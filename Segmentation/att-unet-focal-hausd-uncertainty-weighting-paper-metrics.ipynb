{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8592969,"sourceType":"datasetVersion","datasetId":5140204}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":3446.258833,"end_time":"2024-06-12T13:51:37.411513","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-06-12T12:54:11.15268","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom glob import glob\nimport numpy as np\nimport torch\nfrom random import shuffle\nimport random\nfrom numpy import random as npr\nimport pandas as pd\nimport imageio\nfrom PIL import Image\n\nclass MethaneLoader(DataLoader):\n    def __init__(self, device, mode, plume_id, red=False, alli=False, channels=12):\n        self.device = device\n        self.mode = mode\n        self.reduce = red\n        self.channels = channels\n        if mode == \"train\":\n            persist = False\n        else:\n            persist = True\n        \n        if plume_id is not None:\n            self.pos_labels = sorted(\n                glob(\"/kaggle/input/ch4net-dataset/data/{}/label/*/{}.npy\".format(mode, plume_id)))\n#             self.neg_labels = sorted(\n#                 glob(\"/kaggle/input/ch4net-dataset/data/{}/label/pos/{}.npy\".format(mode, plume_id)))\n            self.neg_labels = []\n        else:\n            self.pos_labels = sorted(\n                glob(\"/kaggle/input/ch4net-dataset/data/{}/label/pos/*.npy\".format(mode)))\n            self.neg_labels = sorted(\n                glob(\"/kaggle/input/ch4net-dataset/data/{}/label/neg/*.npy\".format(mode)))\n        self.labels = self.pos_labels+self.neg_labels  # None\n        \n        if not alli:\n            self.sample_labels_and_combine(persist=persist)\n    def sample_labels_and_combine(self, persist=False):\n        \"\"\"\n        Sample a subset of negative labels for each epoch\n        \"\"\"\n        # if self.mode == \"test\":\n        if self.mode in [\"val\"]:\n            self.labels = self.pos_labels+self.neg_labels\n        else:\n            if persist:\n                random.seed(555)\n            shuffle(self.neg_labels)\n            self.labels = self.pos_labels + \\\n                self.neg_labels[:len(self.pos_labels)]\n            \n    def __len__(self):\n        return len(self.labels)\n        \n    def __getitem__(self, index):\n        f = self.labels[index]\n        # print(f)\n        plume_id = int(f.split(\"/\")[-1].split(\".\")[0])\n        # print(plume_id)\n        target = np.load(f)\n        context = np.load(\n            \"/kaggle/input/ch4net-dataset/data/{}/s2/{}.npy\".format(self.mode, plume_id))\n        if self.channels == 2:\n            context = context[..., 10:]\n        if self.channels == 5:\n            context = np.concatenate(\n                [context[..., 1:4], context[..., 10:]], axis=-1)\n        \n        if self.mode == \"train\":\n            # rotate by 90, 180, 270 degrees\n            degrees = npr.choice([0, 90, 180, 270])\n            context = np.rot90(context, k=degrees//90)\n            target = np.rot90(target, k=degrees//90)\n            if npr.rand() > 0.5:\n                context = np.flip(context, axis=0)\n                target = np.flip(target, axis=0)\n            if npr.rand() > 0.5:\n                context = np.flip(context, axis=1)\n                target = np.flip(target, axis=1)\n        # Crop to centre\n        # x_c = target.shape[0]//2\n        # y_c = target.shape[1]//2\n        s = 64\n        if self.mode == \"train\":\n            rng = npr.RandomState()\n            mid_loc_x = rng.randint(s, target.shape[0]-s)\n            mid_loc_y = rng.randint(s, target.shape[1]-s)\n        else:\n            mid_loc_x = target.shape[0]//2\n            mid_loc_y = target.shape[1]//2\n        target = target[mid_loc_x-s:mid_loc_x+s,\n                        mid_loc_y-s: mid_loc_y+s]\n        context = context[mid_loc_x-s:mid_loc_x+s,\n                          mid_loc_y-s: mid_loc_y+s, :]\n        ### These are not defined before and not necessary for training. \n        ### But these are used in evaluation outputs\n#         diff_img = np.array(diff_img[mid_loc_x-s:mid_loc_x+s,\n#                                      mid_loc_y-s:mid_loc_y+s, :])\n#         diff_img_g = np.array(diff_img_g[mid_loc_x-s:mid_loc_x+s,\n#                                          mid_loc_y-s:mid_loc_y+s])\n#         rgb_img = np.array(rgb_img[mid_loc_x-s:mid_loc_x+s,\n#                                    mid_loc_y-s:mid_loc_y+s, :])\n        if self.reduce:\n            target = np.array([np.int(target.any())])\n        # if self.mode == \"test\":\n            # print(\"Plume ID: {}, date: {}\".format(plume_id, date))\n        d = {\"pred\": torch.from_numpy(context.copy()).float().to(self.device).permute(2, 0, 1)/255,\n             \"target\": torch.from_numpy(target.copy()).float().to(self.device)}\n        \n        # Print the size of the image tensor\n#         print(\"Image tensor size (context):\", d[\"pred\"].size())\n#         print(\"Image tensor size (target):\", d[\"target\"].size())\n        return d","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:28:57.044540Z","iopub.execute_input":"2024-11-04T17:28:57.045374Z","iopub.status.idle":"2024-11-04T17:28:57.067488Z","shell.execute_reply.started":"2024-11-04T17:28:57.045337Z","shell.execute_reply":"2024-11-04T17:28:57.066520Z"},"papermill":{"duration":4.671939,"end_time":"2024-06-12T12:54:19.358125","exception":false,"start_time":"2024-06-12T12:54:14.686186","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MLP(nn.Module):\n    \"\"\"\n    Base MLP module with ReLU activation\n    Parameters:\n    -----------\n    in_channels: Int\n        Number of input channels\n    out_channels: Int\n        Number of output channels\n    h_channels: Int\n        Number of hidden channels\n    h_layers: Int\n        Number of hidden layers\n    \"\"\"\n\n    def __init__(self, \n                in_channels, \n                out_channels, \n                h_channels=64,\n                h_layers=4):\n\n        super().__init__()\n\n        def hidden_block(h_channels):\n            h = nn.Sequential(\n            nn.Linear(h_channels, h_channels),\n            nn.ReLU())\n            return h\n\n        # Model\n        \n        self.mlp = nn.Sequential(\n            nn.Linear(in_channels, h_channels),\n            nn.ReLU(),\n            *[hidden_block(h_channels) for _ in range(h_layers)],\n            nn.Linear(h_channels, out_channels) \n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n\nclass Unet(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 div_factor=8,\n                 prob_output=True,\n                 class_output=False\n\n    ):\n        super(Unet, self).__init__()\n\n        self.n_channels = in_channels\n        self.bilinear = True\n        self.sigmoid = nn.Sigmoid()\n        self.prob_output = prob_output\n        self.class_output = class_output\n\n        def double_conv(in_channels, out_channels):\n            return nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True),\n            )\n\n        def down(in_channels, out_channels):\n            return nn.Sequential(\n                nn.MaxPool2d(2),\n                double_conv(in_channels, out_channels)\n            )\n\n        class up(nn.Module):\n            def __init__(self, in_channels, out_channels, bilinear=True):\n                super().__init__()\n\n                if bilinear:\n                    self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n                else:\n                    self.up = nn.ConvTranpose2d(in_channels // 2, in_channels // 2,\n                                                kernel_size=2, stride=2)\n\n                self.conv = double_conv(in_channels, out_channels)\n\n            def forward(self, x1, x2):\n                x1 = self.up(x1)\n                # [?, C, H, W]\n                diffY = x2.size()[2] - x1.size()[2]\n                diffX = x2.size()[3] - x1.size()[3]\n\n                x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                diffY // 2, diffY - diffY // 2])\n                x = torch.cat([x2, x1], dim=1) ## why 1?\n                return self.conv(x)\n\n        self.inc = double_conv(self.n_channels, 64//div_factor)\n        self.down1 = down(64//div_factor, 128//div_factor)\n        self.down2 = down(128//div_factor, 256//div_factor)\n        self.down3 = down(256//div_factor, 512//div_factor)\n        self.down4 = down(512//div_factor, 512//div_factor)\n        self.up1 = up(1024//div_factor, 256//div_factor)\n        self.up2 = up(512//div_factor, 128//div_factor)\n        self.up3 = up(256//div_factor, 64//div_factor)\n        self.up4 = up(128//div_factor, 128//div_factor)\n        self.out = nn.Conv2d(128//div_factor, 1, kernel_size=1)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n\n        #return self.out(x).permute(0,2,3,1)\n\n        if self.prob_output:\n            x = self.out(x)\n            return self.sigmoid(x).permute(0,2,3,1)\n        else:\n            return self.out(x).permute(0,2,3,1)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:28:57.069424Z","iopub.execute_input":"2024-11-04T17:28:57.070025Z","iopub.status.idle":"2024-11-04T17:28:57.093035Z","shell.execute_reply.started":"2024-11-04T17:28:57.069991Z","shell.execute_reply":"2024-11-04T17:28:57.092115Z"},"papermill":{"duration":0.032909,"end_time":"2024-06-12T12:54:19.395778","exception":false,"start_time":"2024-06-12T12:54:19.362869","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass conv_block(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(conv_block, self).__init__()\n\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\nclass up_conv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(up_conv, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.up(x)\n        return x\n\nclass Attention_block(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super(Attention_block, self).__init__()\n\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, g, x):\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        out = x * psi\n        return out\n\nclass UNet_Attention(nn.Module):\n    def __init__(self, img_ch=12, output_ch=1):\n        super(UNet_Attention, self).__init__()\n\n        n1 = 64\n        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n\n        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.Conv1 = conv_block(img_ch, filters[0])\n        self.Conv2 = conv_block(filters[0], filters[1])\n        self.Conv3 = conv_block(filters[1], filters[2])\n        self.Conv4 = conv_block(filters[2], filters[3])\n        self.Conv5 = conv_block(filters[3], filters[4])\n\n        self.Up5 = up_conv(filters[4], filters[3])\n        self.Att5 = Attention_block(F_g=filters[3], F_l=filters[3], F_int=filters[2])\n        self.Up_conv5 = conv_block(filters[4], filters[3])\n\n        self.Up4 = up_conv(filters[3], filters[2])\n        self.Att4 = Attention_block(F_g=filters[2], F_l=filters[2], F_int=filters[1])\n        self.Up_conv4 = conv_block(filters[3], filters[2])\n\n        self.Up3 = up_conv(filters[2], filters[1])\n        self.Att3 = Attention_block(F_g=filters[1], F_l=filters[1], F_int=filters[0])\n        self.Up_conv3 = conv_block(filters[2], filters[1])\n\n        self.Up2 = up_conv(filters[1], filters[0])\n        self.Att2 = Attention_block(F_g=filters[0], F_l=filters[0], F_int=32)\n        self.Up_conv2 = conv_block(filters[1], filters[0])\n\n        self.Conv = nn.Conv2d(filters[0], output_ch, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x):\n\n        e1 = self.Conv1(x)\n\n        e2 = self.Maxpool1(e1)\n        e2 = self.Conv2(e2)\n\n        e3 = self.Maxpool2(e2)\n        e3 = self.Conv3(e3)\n\n        e4 = self.Maxpool3(e3)\n        e4 = self.Conv4(e4)\n\n        e5 = self.Maxpool4(e4)\n        e5 = self.Conv5(e5)\n\n        d5 = self.Up5(e5)\n\n        x4 = self.Att5(g=d5, x=e4)\n        d5 = torch.cat((x4, d5), dim=1)\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n        x3 = self.Att4(g=d4, x=e3)\n        d4 = torch.cat((x3, d4), dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        x2 = self.Att3(g=d3, x=e2)\n        d3 = torch.cat((x2, d3), dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        x1 = self.Att2(g=d2, x=e1)\n        d2 = torch.cat((x1, d2), dim=1)\n        d2 = self.Up_conv2(d2)\n\n        out = self.Conv(d2)\n\n        return out\n\n# Example usage:\n\n# Instantiate the model\nmodel = UNet_Attention(img_ch=12, output_ch=1)\n\n# Example input tensor with shape [batch_size, channels, height, width]\ninput_tensor = torch.randn(16, 12, 128, 128)  # Example batch size of 16\n\n# Example target tensor with shape [batch_size, height, width]\ntarget_tensor = torch.randn(16, 128, 128)\n\n# Adjust the target tensor to match the output shape\ntarget_tensor = target_tensor.unsqueeze(1)\n\n# Forward pass\noutput = model(input_tensor)\n\n# Check output shape\nprint(f\"Output shape: {output.shape}\")  # Should be [16, 1, 128, 128]\nprint(f\"Target shape: {target_tensor.shape}\")  # Should be [16, 1, 128, 128]","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:28:57.160893Z","iopub.execute_input":"2024-11-04T17:28:57.161400Z","iopub.status.idle":"2024-11-04T17:29:02.600890Z","shell.execute_reply.started":"2024-11-04T17:28:57.161375Z","shell.execute_reply":"2024-11-04T17:29:02.599912Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Output shape: torch.Size([16, 1, 128, 128])\nTarget shape: torch.Size([16, 1, 128, 128])\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n           \n        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n                               nn.ReLU(),\n                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:29:02.602857Z","iopub.execute_input":"2024-11-04T17:29:02.603153Z","iopub.status.idle":"2024-11-04T17:29:02.613473Z","shell.execute_reply.started":"2024-11-04T17:29:02.603128Z","shell.execute_reply":"2024-11-04T17:29:02.612599Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass conv_block(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(conv_block, self).__init__()\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True))\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(out_ch))\n\n    def forward(self, x):\n        x = self.conv1(x) + self.conv2(x)\n        return x\n\nclass up_conv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(up_conv, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.up(x)\n        return x\n\nclass Attention_block(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super(Attention_block, self).__init__()\n\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, g, x):\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        out = x * psi\n        return out\n\nclass Res_UNet_Channel_Attention(nn.Module):\n    def __init__(self, img_ch=12, output_ch=1):\n        super(Res_UNet_Channel_Attention, self).__init__()\n\n        n1 = 64\n        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16, n1 * 32]\n\n        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.Conv1 = conv_block(img_ch, filters[0])\n        self.Conv2 = conv_block(filters[0], filters[1])\n        self.Conv3 = conv_block(filters[1], filters[2])\n        self.Conv4 = conv_block(filters[2], filters[3])\n        self.Conv5 = conv_block(filters[3], filters[4])\n        self.conv6 = conv_block(filters[4], filters[5])\n\n        self.Up6 = up_conv(filters[5], filters[4])\n        self.Att6 = Attention_block(F_g=filters[4], F_l=filters[4], F_int=filters[3])\n        self.Up_conv6 = conv_block(filters[5], filters[4])\n\n        self.Up5 = up_conv(filters[4], filters[3])\n        self.Att5 = Attention_block(F_g=filters[3], F_l=filters[3], F_int=filters[2])\n        self.Up_conv5 = conv_block(filters[4], filters[3])\n\n        self.Up4 = up_conv(filters[3], filters[2])\n        self.Att4 = Attention_block(F_g=filters[2], F_l=filters[2], F_int=filters[1])\n        self.Up_conv4 = conv_block(filters[3], filters[2])\n\n        self.Up3 = up_conv(filters[2], filters[1])\n        self.Att3 = Attention_block(F_g=filters[1], F_l=filters[1], F_int=filters[0])\n        self.Up_conv3 = conv_block(filters[2], filters[1])\n\n        self.Up2 = up_conv(filters[1], filters[0])\n        self.Att2 = Attention_block(F_g=filters[0], F_l=filters[0], F_int=32)\n        self.Up_conv2 = conv_block(filters[1], filters[0])\n\n        self.Conv = nn.Conv2d(filters[0], output_ch, kernel_size=1, stride=1, padding=0)\n        \n        self.Ch_att1 = ChannelAttention(in_planes=filters[0], ratio=8)\n        self.Ch_att2 = ChannelAttention(in_planes=filters[1], ratio=8)\n        self.Ch_att3 = ChannelAttention(in_planes=filters[2], ratio=8)\n        self.Ch_att4 = ChannelAttention(in_planes=filters[3], ratio=8)\n        self.Ch_att5 = ChannelAttention(in_planes=filters[4], ratio=8)\n\n    def forward(self, x):\n\n        e1 = self.Conv1(x)\n\n        e2 = self.Maxpool1(e1)\n        e2 = self.Conv2(e2)\n\n        e3 = self.Maxpool2(e2)\n        e3 = self.Conv3(e3)\n\n        e4 = self.Maxpool3(e3)\n        e4 = self.Conv4(e4)\n\n        e5 = self.Maxpool4(e4)\n        e5 = self.Conv5(e5)\n\n        e6 = self.Maxpool5(e5)\n        e6 = self.conv6(e6)\n\n        d6 = self.Up6(e6)\n\n#         x5 = self.Att6(g=d6, x=self.Ch_att5(e5))\n        x5 = self.Att6(g=d6, x=e5)\n        d6 = torch.cat((x5, d6), dim=1)\n        d6 = self.Up_conv6(d6)\n\n        d5 = self.Up5(d6)\n#         x4 = self.Att5(g=d5, x=self.Ch_att4(e4))\n        x4 = self.Att5(g=d5, x=e4)\n        d5 = torch.cat((x4, d5), dim=1)\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n#         x3 = self.Att4(g=d4, x=self.Ch_att3(e3))\n        x3 = self.Att4(g=d4, x=e3)\n        d4 = torch.cat((x3, d4), dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n#         x2 = self.Att3(g=d3, x=self.Ch_att2(e2))\n        x2 = self.Att3(g=d3, x=e2)\n        d3 = torch.cat((x2, d3), dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n#         x1 = self.Att2(g=d2, x=self.Ch_att1(e1))\n        x1 = self.Att2(g=d2, x=e1)\n        d2 = torch.cat((x1, d2), dim=1)\n        d2 = self.Up_conv2(d2)\n\n        out = self.Conv(d2)\n\n        return out\n\n# Example usage:\n\n# Instantiate the model\nmodel = Res_UNet_Channel_Attention(img_ch=12, output_ch=1)\n\n# Example input tensor with shape [batch_size, channels, height, width]\ninput_tensor = torch.randn(16, 12, 128, 128)  # Example batch size of 16\n\n# Example target tensor with shape [batch_size, height, width]\ntarget_tensor = torch.randn(16, 128, 128)\n\n# Adjust the target tensor to match the output shape\ntarget_tensor = target_tensor.unsqueeze(1)\n\n# Forward pass\noutput = model(input_tensor)\n\n# Check output shape\nprint(f\"Output shape: {output.shape}\")  # Should be [16, 1, 128, 128]\nprint(f\"Target shape: {target_tensor.shape}\")  # Should be [16, 1, 128, 128]","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:29:02.615037Z","iopub.execute_input":"2024-11-04T17:29:02.615341Z","iopub.status.idle":"2024-11-04T17:29:09.914638Z","shell.execute_reply.started":"2024-11-04T17:29:02.615315Z","shell.execute_reply":"2024-11-04T17:29:09.913695Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Output shape: torch.Size([16, 1, 128, 128])\nTarget shape: torch.Size([16, 1, 128, 128])\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport torch\nimport scipy\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch import autograd\nfrom torch.utils.data import DataLoader\n# from loader import *\n\nclass Trainer():\n    \"\"\"\n    Training class for the neural process models\n    \"\"\"\n\n    def __init__(self,\n                 model,\n                 train_loader,\n                 val_loader,\n                 train_dataset,\n                 loss_function,\n                 save_path,\n                 learning_rate):\n\n        # Model and data\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.train_dataset = train_dataset\n        self.save_path = save_path\n\n        # Training parameters\n        self.loss_function = loss_function\n        self.opt = torch.optim.Adam(list(model.parameters()) + list(self.loss_function.parameters()), lr=learning_rate)\n        self.sched = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            self.opt, mode='min', factor=0.5, patience=7, verbose=True)\n\n        # Losses\n        self.losses = []\n        self.mIoUs = []\n\n    def plot_losses(self):\n        \"\"\"\n        Plot losses and IoUs in same figure\n        \"\"\"\n        \n        fig, ax1 = plt.subplots()\n\n        color = 'tab:red'\n        ax1.set_xlabel('Epoch')\n        ax1.set_ylabel('Loss', color=color)\n        ax1.plot(self.losses, color=color)\n        ax1.tick_params(axis='y', labelcolor=color)\n\n        ax2 = ax1.twinx()\n        color = 'tab:blue'\n        ax2.set_ylabel('mIoU', color=color)\n        ax2.plot(self.mIoUs, color=color)\n        ax2.tick_params(axis='y', labelcolor=color)\n\n        fig.tight_layout()\n        plt.show()\n\n    def _unravel_to_numpy(self, x):\n        return x.view(-1).detach().cpu().numpy()\n\n    def eval_epoch(self, verbose=False):\n\n        self.model.eval()\n        lf = []\n        ious = []\n\n        outs = []\n        ts = []\n\n        def calculate_iou(pred, target):\n            eps = 1e-6\n            # convert to sigmoid then to binary\n            pred = F.sigmoid(pred)\n            pred = (pred > 0.5).float()\n            target = target.float()\n            \n            return (torch.sum(pred * target) + eps) / (torch.sum(pred + target) + eps)\n        \n        with torch.no_grad():\n            for task in self.val_loader:\n\n                out = self.model(task[\"pred\"])\n                lf.append(self.loss_function(np.squeeze(out), task[\"target\"]))\n                ious.append(calculate_iou(out, task[\"target\"]))\n                outs.append(out.detach().cpu().numpy())\n                ts.append(task[\"target\"].detach().cpu().numpy())\n\n        # Get loss function\n        log_loss = torch.mean(torch.tensor(lf))\n        print(\"- Log loss: {}\".format(log_loss))\n        mIoU = torch.mean(torch.tensor(ious))\n        print(\"- mIoU: {}\".format(mIoU))\n\n        if verbose:\n            return log_loss, np.concatenate(outs, axis=0), np.concatenate(ts, axis=0)\n\n        return log_loss, mIoU\n\n    def train(self, n_epochs=100):\n\n        # Init progress bar\n        best_loss = 100000\n        best_model_state_dict = None\n        best_opt_state_dict = None\n        i = 1\n\n        for epoch in range(n_epochs):\n\n            autograd.set_detect_anomaly(True)\n\n            print(\"Training epoch {}\".format(epoch))\n\n            if epoch < 500:\n                self.model.train()\n                self.train_dataset.sample_labels_and_combine()\n                self.train_loader = DataLoader(self.train_dataset,\n                                               batch_size=16,\n                                               shuffle=True)\n\n            with tqdm(self.train_loader, unit=\"batch\") as tepoch:\n                for task in tepoch:\n\n                    out = self.model(task[\"pred\"])\n\n                    # print(\"loss\")\n                    loss = self.loss_function(np.squeeze(out), task[\"target\"])\n\n                    loss.backward()\n                    self.opt.step()\n                    self.opt.zero_grad()\n                    # print(\"out\")\n                    tepoch.set_postfix(loss=loss.item())\n                epoch_loss, mIoU = self.eval_epoch(verbose=False)\n                self.sched.step(epoch_loss)\n#                 print(self.sched.get_last_lr())\n\n            if np.logical_or(epoch_loss <= best_loss, epoch >= 1000):\n                #                 torch.save({\n                #                     'epoch': epoch,\n                #                     'model_state_dict': self.model.state_dict(),\n                #                     'optimizer_state_dict': self.opt.state_dict(),\n                #                     'loss': epoch_loss\n                #                 }, self.save_path+\"epoch_{}\".format(epoch))\n                best_model_state_dict = self.model.state_dict()\n                best_opt_state_dict = self.opt.state_dict()\n                best_loss = epoch_loss\n#                 np.save(\"outs.npy\", o)\n#                 np.save(\"ts.npy\", t)\n\n            self.losses.append(epoch_loss)\n            self.mIoUs.append(mIoU)\n\n            # checkpoint\n            if i % 20 == 0 or i == n_epochs:\n                np.save(self.save_path+\"losses.npy\", np.array(self.losses))\n                self.plot_losses()\n\n                torch.save({\n                    # 'epoch': epoch,\n                    'model_state_dict': best_model_state_dict,\n                    'optimizer_state_dict': best_opt_state_dict,\n                    'loss': best_loss\n                }, self.save_path+\"final_model\".format(epoch))\n            i += 1\n\n        print(\"Training complete!\")","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:29:09.916411Z","iopub.execute_input":"2024-11-04T17:29:09.916788Z","iopub.status.idle":"2024-11-04T17:29:09.941997Z","shell.execute_reply.started":"2024-11-04T17:29:09.916753Z","shell.execute_reply":"2024-11-04T17:29:09.941087Z"},"papermill":{"duration":0.026808,"end_time":"2024-06-12T12:54:19.427168","exception":false,"start_time":"2024-06-12T12:54:19.40036","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":"!mkdir -p /kaggle/working/train_out\n!mkdir -p /kaggle/working/eval_out","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:29:09.944475Z","iopub.execute_input":"2024-11-04T17:29:09.944786Z","iopub.status.idle":"2024-11-04T17:29:12.173795Z","shell.execute_reply.started":"2024-11-04T17:29:09.944762Z","shell.execute_reply":"2024-11-04T17:29:12.172504Z"},"papermill":{"duration":1.939715,"end_time":"2024-06-12T12:54:21.37123","exception":false,"start_time":"2024-06-12T12:54:19.431515","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"!pip install monai","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:29:12.175885Z","iopub.execute_input":"2024-11-04T17:29:12.176376Z","iopub.status.idle":"2024-11-04T17:29:24.498704Z","shell.execute_reply.started":"2024-11-04T17:29:12.176332Z","shell.execute_reply":"2024-11-04T17:29:24.497484Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: monai in /opt/conda/lib/python3.10/site-packages (1.4.0)\nRequirement already satisfied: numpy<2.0,>=1.24 in /opt/conda/lib/python3.10/site-packages (from monai) (1.26.4)\nRequirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.10/site-packages (from monai) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (2024.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9->monai) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9->monai) (1.3.0)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.ops import sigmoid_focal_loss\nfrom monai.losses.hausdorff_loss import HausdorffDTLoss\n\ndef loss(pred, target):\n#     print(pred.shape, target.shape)\n    bce_loss = nn.BCEWithLogitsLoss(reduction=\"none\") \n    ll = bce_loss(pred, target)\n\n    ll = ll.sum(dim=(-2,-1)) #*mask\n    return ll.mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.75, gamma=2.0, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n        self.bce_loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n    \n    def forward(self, pred, target):\n        loss = self.bce_loss(pred, target)\n        prob = torch.sigmoid(pred)  # Predicted probability\n        alpha = torch.where(target == 1, self.alpha, 1 - self.alpha)  # Class balancing factor\n        focal_weight = torch.where(target == 1, 1 - prob, prob)  # Focusing weight\n        focal_weight = alpha * focal_weight**self.gamma  # Apply alpha and gamma\n        focal_loss = focal_weight * loss\n        \n        focal_loss = focal_loss.sum(dim=(-2,-1)) #*mask\n        \n        return focal_loss.mean() if self.reduction == 'mean' else focal_loss\n\n    \nclass MultiScalePoolingLoss(nn.Module):\n    def __init__(self, alpha=0.75, gamma=2, itr=3, ratio=None):\n        super(MultiScalePoolingLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.itr = itr\n        self.focal_loss = FocalLoss(alpha=alpha, gamma=gamma)\n        self.ratio = torch.tensor(ratio)\n    \n    def forward(self, pred, target):\n        losses = torch.zeros(self.itr)\n        if(pred.dim() == 2):\n            pred = pred.unsqueeze(0)\n            target = target.unsqueeze(0)\n        elif(pred.dim() == 3):\n            pred = pred.unsqueeze(1)\n            target = target.unsqueeze(1)\n        for i in range(self.itr):\n            losses[i] = self.focal_loss(pred, target)\n            pred = F.max_pool2d(pred, kernel_size=2, stride=2)\n            target = F.max_pool2d(target, kernel_size=2, stride=2)\n        \n        if self.ratio is not None:\n            losses = losses * self.ratio\n        else:\n            losses = losses * 1/self.itr\n        \n        return torch.sum(losses)\n\n    \nclass DiceBCELoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceBCELoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        # Comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)\n\n        # Flatten label and prediction tensors\n        inputs = inputs.view(inputs.size(0), -1)\n        targets = targets.view(targets.size(0), -1)\n        \n        # Compute Dice loss\n        intersection = (inputs * targets).sum(dim=1)\n        dice_loss = 1 - (2.0 * intersection + smooth) / (inputs.sum(dim=1) + targets.sum(dim=1) + smooth)\n\n        # Compute BCE loss\n        BCE = F.binary_cross_entropy(inputs, targets, reduction='none')\n        BCE = BCE.mean(dim=1)\n\n        # Combine Dice and BCE losses\n        Dice_BCE = BCE + dice_loss\n\n        return Dice_BCE.mean()\n\n    \nclass HausdorffDT_Loss(nn.Module):\n    def __init__(self):\n        super(HausdorffDT_Loss, self).__init__()\n        self.hd_loss = HausdorffDTLoss(reduction='mean', sigmoid=True)\n        self.focal_loss = FocalLoss\n\n    def forward(self, inputs, targets):\n        # Comment out if your model contains a sigmoid or equivalent activation layer\n        return self.hd_loss(inputs.unsqueeze(1), targets.unsqueeze(1))\n\n    \n# UncertaintyWeighting\n# This method learns to balance multiple losses by considering the homoscedastic uncertainty of each task. \n# It automatically adjusts the relative weights of the losses during training\nclass HausdorffDT_Focal_Loss(nn.Module):\n    def __init__(self, focal_weight = 0.5, hausdorff_weight = 0.5):\n        super(HausdorffDT_Focal_Loss, self).__init__()\n        self.hd_loss = HausdorffDTLoss(reduction='none', sigmoid=True)\n        self.focal_loss = FocalLoss(alpha=0.75, gamma=2.0, reduction='none')\n        self.log_vars = nn.Parameter(torch.zeros(2))\n\n    def forward(self, inputs, targets):\n        # Comment out if your model contains a sigmoid or equivalent activation layer\n        hl = self.hd_loss(inputs.unsqueeze(1), targets.unsqueeze(1))\n        fl = self.focal_loss(inputs, targets)\n        \n        precision1 = torch.exp(-self.log_vars[0])\n        loss1 = precision1 * fl + self.log_vars[0]\n        \n        precision2 = torch.exp(-self.log_vars[1])\n        loss2 = precision2 * hl + self.log_vars[1]\n        \n        return torch.mean(loss1 + loss2)\n    \n\npred = torch.randn(16, 1, 128, 128).float()\ntarget = torch.randint(0, 2, (16, 1, 128, 128)).float()\n\n# focal_loss = FocalLoss(alpha=0.75, gamma=1.0)\n# hausdorffDTLoss = HausdorffDT_Loss(reduction='none', sigmoid=True)\n# dbceloss = DiceBCELoss()\n# multi_scale_pooling_loss = MultiScalePoolingLoss(alpha=0.75, gamma=2.0, itr=3, ratio=[0.5, 0.3, 0.2])\n# hausdorffDT_focal_loss = HausdorffDT_Focal_Loss(focal_weight = 0.65)\n\n# focal_loss(pred, target)\n# multi_scale_pooling_loss(pred, target)\n# print(hausdorffDT_focal_loss(pred, target))","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:29:24.500571Z","iopub.execute_input":"2024-11-04T17:29:24.500919Z","iopub.status.idle":"2024-11-04T17:29:24.538705Z","shell.execute_reply.started":"2024-11-04T17:29:24.500890Z","shell.execute_reply":"2024-11-04T17:29:24.537766Z"},"trusted":true},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# from models import *\n# from trainer import *\n# from loader import *\nfrom torch.distributions.bernoulli import Bernoulli\nfrom torch.distributions.multivariate_normal import MultivariateNormal\nimport sys\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom torch.nn import DataParallel\n\n#python3 train.py 12 FINAL_12/\n\n# Input arguments\n# channels = int(sys.argv[1])\n# out_dir = sys.argv[2]\n\nchannels = 12\nout_dir = '/kaggle/working/train_out/'\n\n# Set up \ntorch.manual_seed(0)\ntorch.backends.cudnn.benchmark = True\ndevice = torch.device('cuda')\n\n# Set up model\nmodel = UNet_Attention(img_ch=channels,\n            output_ch=1,\n            )\n# model = Unet(in_channels=channels,\n#             out_channels=1,\n#             div_factor=1, \n#             prob_output=False)\n# model = Res_UNet_Channel_Attention(img_ch=12, output_ch=1)\nmodel = model.to(device)\nmodel = nn.DataParallel(model)\n\n# Set up loss function\n# loss_fn = MultiScalePoolingLoss(alpha=0.75, gamma=0.0, itr=4, ratio=[0.4, 0.2, 0.2, 0.3])\nloss_fn = HausdorffDT_Focal_Loss(focal_weight = 0.65)\n\ntrain_dataset = MethaneLoader(device = \"cuda\", mode=\"train\", plume_id=None, channels=channels)\ntest_dataset = MethaneLoader(device = \"cuda\", mode=\"test\", plume_id=None, channels=channels)\n\n#print(train_dataset.__len__())\n\ntrain_loader = DataLoader(train_dataset, \n                          batch_size = 16, \n                          shuffle = True)\n\ntest_loader = DataLoader(test_dataset, \n                          batch_size = 16, \n                          shuffle = True)\n\n# Make the trainer\ntrainer = Trainer(model,\n                  train_loader,\n                  test_loader,\n                  train_dataset,\n                  loss_fn,\n                  out_dir,\n                  1e-4)\n\n# Train\ntrainer.train(n_epochs=1)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:29:24.540095Z","iopub.execute_input":"2024-11-04T17:29:24.540415Z","iopub.status.idle":"2024-11-04T17:30:08.682702Z","shell.execute_reply.started":"2024-11-04T17:29:24.540391Z","shell.execute_reply":"2024-11-04T17:30:08.681690Z"},"papermill":{"duration":3349.124987,"end_time":"2024-06-12T13:50:10.500691","exception":false,"start_time":"2024-06-12T12:54:21.375704","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Training epoch 0\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/75 [00:00<?, ?batch/s]/opt/conda/lib/python3.10/site-packages/monai/losses/hausdorff_loss.py:171: UserWarning: single channel prediction, `include_background=False` ignored.\n  warnings.warn(\"single channel prediction, `include_background=False` ignored.\")\n100%|██████████| 75/75 [00:36<00:00,  2.07batch/s, loss=289]    \n","output_type":"stream"},{"name":"stdout","text":"- Log loss: 310.10333251953125\n- mIoU: 0.0010853138519451022\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"Training complete!\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"### for clearing directory\n### skip it\n#!rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:30:08.684207Z","iopub.execute_input":"2024-11-04T17:30:08.684545Z","iopub.status.idle":"2024-11-04T17:30:08.688549Z","shell.execute_reply.started":"2024-11-04T17:30:08.684517Z","shell.execute_reply":"2024-11-04T17:30:08.687543Z"},"papermill":{"duration":1.833333,"end_time":"2024-06-12T13:50:14.203088","exception":false,"start_time":"2024-06-12T13:50:12.369755","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def visualize(idx):\n    file_paths = glob('/kaggle/input/ch4net-dataset/data/val/mbmp/*/{}.npy'.format(idx))\n    for file in file_paths:\n        category = file.split(\"/\")[-2]\n        print(category)\n        image1 = np.load(file)\n    image2 = np.load(\"/kaggle/working/eval_out/target_{}.npy\".format(idx))\n    image3 = np.load(\"/kaggle/working/eval_out/out_{}.npy\".format(idx))\n\n    # Squeeze the extra dimension if needed\n    image1 = np.squeeze(image1)\n    image2 = np.squeeze(image2)\n    image3 = np.squeeze(image3)\n\n    print(image1.shape, image2.shape, image3.shape)\n\n    # Create a figure with 3 subplots side by side\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    # Display each image\n    axes[0].imshow(image1, cmap='magma')\n    axes[0].set_title('MBMP')\n    axes[0].axis('off')  # Hide axis\n\n    axes[1].imshow(image2, cmap='magma')\n    axes[1].set_title('Target')\n    axes[1].axis('off')  # Hide axis\n\n    axes[2].imshow(image3, cmap='magma')\n    axes[2].set_title('Out')\n    axes[2].axis('off')  # Hide axis\n\n    # Show the figure\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:30:08.690055Z","iopub.execute_input":"2024-11-04T17:30:08.690354Z","iopub.status.idle":"2024-11-04T17:30:08.702000Z","shell.execute_reply.started":"2024-11-04T17:30:08.690329Z","shell.execute_reply":"2024-11-04T17:30:08.701213Z"},"papermill":{"duration":1.882555,"end_time":"2024-06-12T13:50:17.953892","exception":false,"start_time":"2024-06-12T13:50:16.071337","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":39},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\n\n# from models import *\n# from trainer import *\n# from loader import *\n\nfrom torch.utils.data import DataLoader\nfrom glob import glob\n\n# python3 gen_eval_preds.py 12 final_12_all/ final_12_preds/ 0\n# in_dir = sys.argv[2]\n# out_dir = sys.argv[3]\n# channels = int(sys.argv[1])\n# alli_yn = bool(int(sys.argv[4]))\n\nin_dir = \"/kaggle/working/train_out/\"\nout_dir = \"/kaggle/working/eval_out/\"\nchannels = 12\nalli_yn = False\n\nprint(\"Loading losses...\")\nlosses = np.load(in_dir+\"losses.npy\")\nbest_epoch = np.argmin(losses)\nprint('best epoch: ', best_epoch)\n\ndevice = torch.device('cuda')\n\nmodel = UNet_Attention(img_ch=channels,\n            output_ch=1,\n            )\n# model = Unet(in_channels=channels,\n#             out_channels=1,\n#             div_factor=1, \n#             prob_output=False)\n# model = Res_UNet_Channel_Attention(img_ch=12, output_ch=1)\nmodel = model.to(device)\nmodel = nn.DataParallel(model)\n\nmodel.load_state_dict(torch.load(in_dir+\"final_model\",\n                      map_location=torch.device('cuda'))[\"model_state_dict\"])\n# model.load_state_dict(torch.load('/kaggle/input/ch4net_attention/pytorch/default/1/final_model',\n#                       map_location=torch.device('cuda'))[\"model_state_dict\"])\nmodel.eval()\nprint()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T17:30:08.703062Z","iopub.execute_input":"2024-11-04T17:30:08.703404Z","iopub.status.idle":"2024-11-04T17:30:09.394940Z","shell.execute_reply.started":"2024-11-04T17:30:08.703379Z","shell.execute_reply":"2024-11-04T17:30:09.393972Z"},"papermill":{"duration":17.430864,"end_time":"2024-06-12T13:50:37.283462","exception":false,"start_time":"2024-06-12T13:50:19.852598","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Loading losses...\nbest epoch:  0\n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"val_dataset = MethaneLoader(device=\"cuda\", mode=\"val\", alli=False, plume_id=None, channels=12)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\nfor i, batch in tqdm(enumerate(val_loader)):\n    with torch.no_grad():\n        pred = model(batch[\"pred\"])\n    if i == 0:\n        out = pred\n        target = batch[\"target\"]\n    else:\n        out = torch.cat((out, pred), dim=0)\n        target = torch.cat((target, batch[\"target\"]), dim=0)\n\nout = out.detach().cpu().numpy()\ntarget = target.detach().cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T17:30:09.396315Z","iopub.execute_input":"2024-11-04T17:30:09.397050Z"}},"outputs":[{"name":"stderr","text":"6it [00:00,  7.46it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom sklearn.metrics import balanced_accuracy_score\n\ndef calculate_metrics(pred, target, epsilon=1e-6):\n    \"\"\"\n    Calculate IoU, Accuracy, Recall, Balanced Accuracy, False Positive Rate, and False Negative Rate over batch\n    \"\"\"\n    pred = F.sigmoid(torch.tensor(pred)).numpy()\n    pred = pred > 0.5 # Convert to binary mask\n    target = np.expand_dims(target, axis=1) > 0  # Convert to binary mask\n    \n    # for (batch, 1, 128, 128) -> (batch, 128*128) true positive, true negative, false positive, false negative\n    tp = np.sum(pred * target, axis=(1, 2, 3))\n    tn = np.sum((1 - pred) * (1 - target), axis=(1, 2, 3))\n    fp = np.sum(pred * (1 - target), axis=(1, 2, 3))\n    fn = np.sum((1 - pred) * target, axis=(1, 2, 3))\n\n    iou = np.mean(tp / (tp + fp + fn + epsilon))\n    accuracy = np.mean((tp + tn) / (tp + tn + fp + fn + epsilon))\n    recall = np.mean(tp / (tp + fn + epsilon))\n    balanced_acc = balanced_accuracy_score(target.flatten(), pred.flatten())\n    fpr = np.mean(fp / (fp + tn + epsilon))\n    fnr = np.mean(fn / (fn + tp + epsilon))\n        \n    return iou, accuracy, recall, balanced_acc, fpr, fnr\n\niou, accuracy, recall, balanced_acc, fpr, fnr = calculate_metrics(out, target)\n\nmetric_df = pd.DataFrame({\n    \"Metric\": [\"IoU\", \"Accuracy\", \"Recall\", \"Balanced Accuracy\", \"False Positive Rate\", \"False Negative Rate\"],\n    \"Value\": [iou, accuracy, recall, balanced_acc, fpr, fnr]\n})\n\nmetric_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom sklearn.metrics import balanced_accuracy_score\n\ndef calculate_scene_metrics(pred, target, epsilon=1e-6):\n    pred = F.sigmoid(torch.tensor(pred)).numpy()\n    pred = pred > 0.25 # Convert to binary mask\n    target = np.expand_dims(target, axis=1) > 0  # Convert to binary mask\n\n    pred_binary = np.sum(pred, axis=(1, 2, 3)) >= 115\n    target_binary = np.sum(target, axis=(1, 2, 3)) >= 115\n\n    true_positive = np.sum(np.logical_and(pred_binary, target_binary))\n    false_positive = np.sum(np.logical_and(pred_binary, np.logical_not(target_binary)))\n    false_negative = np.sum(np.logical_and(np.logical_not(pred_binary), target_binary))\n    true_negative = np.sum(np.logical_and(np.logical_not(pred_binary), np.logical_not(target_binary)))\n\n    # Calculate Accuracy, Recall, False Positive Rate, and False Negative Rate\n    total = pred_binary.size\n    accuracy = (true_positive + true_negative) / total\n    recall = (true_positive + epsilon) / (true_positive + false_negative + epsilon)\n    fpr = (false_positive + epsilon) / (false_positive + true_negative + epsilon)\n    fnr = (false_negative + epsilon) / (false_negative + true_positive + epsilon)\n    balanced_acc = balanced_accuracy_score(target_binary, pred_binary)\n\n    return accuracy, recall, balanced_acc, fpr, fnr\n    \n\ndef calculate_pixel_metrics(pred, target, epsilon=1e-6):\n    pred = F.sigmoid(torch.tensor(pred)).numpy()\n    pred = pred > 0.25 # Convert to binary mask\n    target = np.expand_dims(target, axis=1) > 0  # Convert to binary mask\n\n    true_positive = np.sum(np.logical_and(pred, target))\n    \n    # calculate IoU, balanced accuracy\n    union = np.sum(np.logical_or(pred, target))\n    iou = (true_positive + epsilon) / (union + epsilon)\n    balanced_acc = balanced_accuracy_score(target.flatten(), pred.flatten())\n\n    return iou, balanced_acc\n\naccuracy, recall, balanced_acc, fpr, fnr = calculate_scene_metrics(out, target)\niou, balanced_acc_pixel = calculate_pixel_metrics(out, target)\n\nmetric_df = pd.DataFrame({\n    \"Metric\": [\"Accuracy\", \"Recall\", \"Balanced Accuracy\", \"False Positive Rate\", \"False Negative Rate\", \"IoU\", \"Balanced Accuracy (Pixel)\"],\n    \"Value\": [accuracy, recall, balanced_acc, fpr, fnr, iou, balanced_acc_pixel]\n})\n\nmetric_df","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
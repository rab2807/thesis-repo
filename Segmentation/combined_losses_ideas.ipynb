{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive weighting based on loss gradients:\n",
    "\n",
    "Adaptive Loss CombinationClick to open code\n",
    "This approach uses the gradients of each loss with respect to a learnable parameter to determine their relative importance. The weight is determined by the ratio of these gradients, ensuring that the loss with the larger gradient (and thus potentially more room for improvement) gets more weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AdaptiveLossCombination(nn.Module):\n",
    "    def __init__(self, alpha=0.5, momentum=0.9):\n",
    "        super(AdaptiveLossCombination, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.tensor(alpha))\n",
    "        self.momentum = momentum\n",
    "        self.register_buffer('moving_focal', torch.tensor(0.0))\n",
    "        self.register_buffer('moving_hausdorff', torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, focal_loss, hausdorff_loss):\n",
    "        # Ensure losses are scalars\n",
    "        focal_loss = torch.mean(focal_loss)\n",
    "        hausdorff_loss = torch.mean(hausdorff_loss)\n",
    "\n",
    "        # Update moving averages\n",
    "        self.moving_focal = self.momentum * self.moving_focal + (1 - self.momentum) * focal_loss.detach()\n",
    "        self.moving_hausdorff = self.momentum * self.moving_hausdorff + (1 - self.momentum) * hausdorff_loss.detach()\n",
    "\n",
    "        # Compute relative loss magnitudes\n",
    "        total_loss = self.moving_focal + self.moving_hausdorff\n",
    "        focal_ratio = self.moving_focal / total_loss\n",
    "        hausdorff_ratio = self.moving_hausdorff / total_loss\n",
    "\n",
    "        # Adjust alpha based on moving averages\n",
    "        self.alpha.data = torch.clamp(hausdorff_ratio, 0.1, 0.9)\n",
    "\n",
    "        # Compute weighted loss\n",
    "        loss = self.alpha * focal_loss + (1 - self.alpha) * hausdorff_loss\n",
    "\n",
    "        return loss, self.alpha.item()\n",
    "\n",
    "# Usage\n",
    "adaptive_loss = AdaptiveLossCombination()\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(adaptive_loss.parameters()))\n",
    "\n",
    "# In your training loop\n",
    "focal_loss = compute_focal_loss(predictions, targets)\n",
    "hausdorff_loss = compute_hausdorff_loss(predictions, targets)\n",
    "\n",
    "loss, weight = adaptive_loss(focal_loss, hausdorff_loss)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainty weighting:\n",
    "\n",
    "Uncertainty Weighting for Loss CombinationClick to open code\n",
    "This method learns to balance multiple losses by considering the homoscedastic uncertainty of each task. It automatically adjusts the relative weights of the losses during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly. The uncertainty weighting loss, also known as homoscedastic uncertainty weighting, is an interesting approach to combining multiple losses in multi-task learning scenarios. Let's dive deeper into how it works and why it's effective.\n",
    "How it works:\n",
    "\n",
    "Basic Principle:\n",
    "The method assigns a learnable weight to each loss term. These weights are interpreted as the inverse of the variance (or uncertainty) of each task.\n",
    "Mathematical Formulation:\n",
    "For two tasks with losses L1 and L2, the combined loss L is formulated as:\n",
    "L = L1 / (2 * σ1^2) + L2 / (2 * σ2^2) + log(σ1 * σ2)\n",
    "Here, σ1 and σ2 are learnable parameters representing the task-dependent uncertainties.\n",
    "Implementation Details:\n",
    "\n",
    "Instead of directly learning σ, we learn log(σ^2) for numerical stability.\n",
    "The loss for each task is multiplied by exp(-log(σ^2)) = 1 / σ^2.\n",
    "We add log(σ) to the loss to prevent the uncertainty from becoming too large.\n",
    "\n",
    "\n",
    "Automatic Balancing:\n",
    "\n",
    "If a task's uncertainty increases, its weight in the total loss decreases.\n",
    "Conversely, if a task's uncertainty decreases, its weight in the total loss increases.\n",
    "\n",
    "\n",
    "\n",
    "Why it works:\n",
    "\n",
    "- Adaptive Weighting:\n",
    "The method automatically adjusts the relative weights of different losses during training. This is particularly useful when the losses are on different scales or have different units.\n",
    "- Principled Approach:\n",
    "It's grounded in the probabilistic interpretation of model outputs, viewing the task of balancing losses as a maximum likelihood problem with gaussian likelihood.\n",
    "- Regularization Effect:\n",
    "The log(σ) term acts as a regularizer, preventing the model from ignoring any of the tasks by making their associated uncertainty very large.\n",
    "- Task Difficulty Consideration:\n",
    "It inherently accounts for the difficulty of each task. A task that's harder to learn or has more noise in its labels will naturally have higher uncertainty, reducing its impact on the overall loss.\n",
    "- No Manual Tuning:\n",
    "Unlike fixed weighting schemes, this method doesn't require manual tuning of loss weights, which can be time-consuming and suboptimal.\n",
    "- Interpretability:\n",
    "The learned uncertainties provide insight into the relative difficulty or noise level of different tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UncertaintyWeighting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UncertaintyWeighting, self).__init__()\n",
    "        self.log_vars = nn.Parameter(torch.zeros(2))\n",
    "\n",
    "    def forward(self, focal_loss, hausdorff_loss):\n",
    "        precision1 = torch.exp(-self.log_vars[0])\n",
    "        loss1 = precision1 * focal_loss + self.log_vars[0]\n",
    "\n",
    "        precision2 = torch.exp(-self.log_vars[1])\n",
    "        loss2 = precision2 * hausdorff_loss + self.log_vars[1]\n",
    "\n",
    "        return loss1 + loss2\n",
    "\n",
    "# Usage\n",
    "uncertainty_loss = UncertaintyWeighting()\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(uncertainty_loss.parameters()))\n",
    "\n",
    "# In your training loop\n",
    "focal_loss = compute_focal_loss(predictions, targets)\n",
    "hausdorff_loss = compute_hausdorff_loss(predictions, targets)\n",
    "\n",
    "combined_loss = uncertainty_loss(focal_loss, hausdorff_loss)\n",
    "combined_loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Periodic alternating focus:\n",
    "\n",
    "Instead of trying to combine the losses, you could alternate between focusing on one loss or the other. This doesn't require a code snippet, but here's how you might implement it:\n",
    "This approach allows the model to focus on optimizing one loss at a time, potentially leading to better overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your training loop\n",
    "if epoch % 2 == 0:\n",
    "    loss = focal_loss\n",
    "else:\n",
    "    loss = hausdorff_loss\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-objective optimization:\n",
    "\n",
    "You could treat this as a multi-objective optimization problem and use techniques like Pareto optimization. This is more complex and would require restructuring your training loop, but it can be very effective for balancing multiple objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss annealing:\n",
    "\n",
    "Start with one loss (e.g., focal loss) and gradually introduce the other loss (Hausdorff loss) over time. This allows the model to first learn the basic task before refining its performance with the second loss.\n",
    "\n",
    "These approaches offer different ways to combine or balance your losses. The effectiveness of each method can vary depending on your specific task and dataset. I recommend experimenting with these approaches to see which works best for your image segmentation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your training loop\n",
    "annealing_factor = min(1.0, current_epoch / total_epochs)\n",
    "loss = focal_loss + annealing_factor * hausdorff_loss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

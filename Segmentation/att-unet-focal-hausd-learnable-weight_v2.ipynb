{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8592969,"sourceType":"datasetVersion","datasetId":5140204}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":3446.258833,"end_time":"2024-06-12T13:51:37.411513","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-06-12T12:54:11.15268","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nfrom PIL import Image\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom glob import glob\nimport numpy as np\nimport torch\nfrom random import shuffle\nimport random\nfrom numpy import random as npr\nimport pandas as pd\nimport imageio\nfrom PIL import Image\n\n\nclass MethaneLoader(DataLoader):\n\n    def __init__(self, device, mode, plume_id, red=False, alli=False, channels=12):\n        self.device = device\n        self.mode = mode\n        self.reduce = red\n        self.channels = channels\n\n        if mode == \"train\":\n            persist = False\n        else:\n            persist = True\n        \n        if plume_id is not None:\n            self.pos_labels = sorted(\n                glob(\"/kaggle/input/ch4net-dataset/data/{}/label/*/{}.npy\".format(mode, plume_id)))\n#             self.neg_labels = sorted(\n#                 glob(\"/kaggle/input/ch4net-dataset/data/{}/label/pos/{}.npy\".format(mode, plume_id)))\n            self.neg_labels = []\n        else:\n            self.pos_labels = sorted(\n                glob(\"/kaggle/input/ch4net-dataset/data/{}/label/pos/*.npy\".format(mode)))\n            self.neg_labels = sorted(\n                glob(\"/kaggle/input/ch4net-dataset/data/{}/label/neg/*.npy\".format(mode)))\n\n        self.labels = self.pos_labels+self.neg_labels  # None\n        \n        if not alli:\n            self.sample_labels_and_combine(persist=persist)\n\n    def sample_labels_and_combine(self, persist=False):\n        \"\"\"\n        Sample a subset of negative labels for each epoch\n        \"\"\"\n        # if self.mode == \"test\":\n        if self.mode in [\"val\"]:\n            self.labels = self.pos_labels+self.neg_labels\n        else:\n            if persist:\n                random.seed(555)\n\n            shuffle(self.neg_labels)\n            self.labels = self.pos_labels + \\\n                self.neg_labels[:len(self.pos_labels)]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, index):\n\n        f = self.labels[index]\n        # print(f)\n\n        plume_id = int(f.split(\"/\")[-1].split(\".\")[0])\n        # print(plume_id)\n\n        target = np.load(f)\n        context = np.load(\n            \"/kaggle/input/ch4net-dataset/data/{}/s2/{}.npy\".format(self.mode, plume_id))\n\n        if self.channels == 2:\n            context = context[..., 10:]\n        if self.channels == 5:\n            context = np.concatenate(\n                [context[..., 1:4], context[..., 10:]], axis=-1)\n        \n        if self.mode == \"train\":\n            # rotate by 90, 180, 270 degrees\n            degrees = npr.choice([0, 90, 180, 270])\n            context = np.rot90(context, k=degrees//90)\n            target = np.rot90(target, k=degrees//90)\n            if npr.rand() > 0.5:\n                context = np.flip(context, axis=0)\n                target = np.flip(target, axis=0)\n            if npr.rand() > 0.5:\n                context = np.flip(context, axis=1)\n                target = np.flip(target, axis=1)\n\n        # Crop to centre\n        # x_c = target.shape[0]//2\n        # y_c = target.shape[1]//2\n        s = 64\n\n        if self.mode == \"train\":\n            rng = npr.RandomState()\n            mid_loc_x = rng.randint(s, target.shape[0]-s)\n            mid_loc_y = rng.randint(s, target.shape[1]-s)\n\n        else:\n            mid_loc_x = target.shape[0]//2\n            mid_loc_y = target.shape[1]//2\n\n        target = target[mid_loc_x-s:mid_loc_x+s,\n                        mid_loc_y-s: mid_loc_y+s]\n\n        context = context[mid_loc_x-s:mid_loc_x+s,\n                          mid_loc_y-s: mid_loc_y+s, :]\n\n        ### These are not defined before and not necessary for training. \n        ### But these are used in evaluation outputs\n#         diff_img = np.array(diff_img[mid_loc_x-s:mid_loc_x+s,\n#                                      mid_loc_y-s:mid_loc_y+s, :])\n#         diff_img_g = np.array(diff_img_g[mid_loc_x-s:mid_loc_x+s,\n#                                          mid_loc_y-s:mid_loc_y+s])\n#         rgb_img = np.array(rgb_img[mid_loc_x-s:mid_loc_x+s,\n#                                    mid_loc_y-s:mid_loc_y+s, :])\n\n        if self.reduce:\n            target = np.array([np.int(target.any())])\n\n        # if self.mode == \"test\":\n            # print(\"Plume ID: {}, date: {}\".format(plume_id, date))\n\n        d = {\"pred\": torch.from_numpy(context.copy()).float().to(self.device).permute(2, 0, 1)/255,\n             \"target\": torch.from_numpy(target.copy()).float().to(self.device)}\n        \n        # Print the size of the image tensor\n#         print(\"Image tensor size (context):\", d[\"pred\"].size())\n#         print(\"Image tensor size (target):\", d[\"target\"].size())\n\n        return d","metadata":{"papermill":{"duration":4.671939,"end_time":"2024-06-12T12:54:19.358125","exception":false,"start_time":"2024-06-12T12:54:14.686186","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-09T21:12:31.361610Z","iopub.execute_input":"2024-09-09T21:12:31.362065Z","iopub.status.idle":"2024-09-09T21:12:36.463350Z","shell.execute_reply.started":"2024-09-09T21:12:31.362025Z","shell.execute_reply":"2024-09-09T21:12:36.462531Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MLP(nn.Module):\n    \"\"\"\n    Base MLP module with ReLU activation\n    Parameters:\n    -----------\n    in_channels: Int\n        Number of input channels\n    out_channels: Int\n        Number of output channels\n    h_channels: Int\n        Number of hidden channels\n    h_layers: Int\n        Number of hidden layers\n    \"\"\"\n\n    def __init__(self, \n                in_channels, \n                out_channels, \n                h_channels=64,\n                h_layers=4):\n\n        super().__init__()\n\n        def hidden_block(h_channels):\n            h = nn.Sequential(\n            nn.Linear(h_channels, h_channels),\n            nn.ReLU())\n            return h\n\n        # Model\n        \n        self.mlp = nn.Sequential(\n            nn.Linear(in_channels, h_channels),\n            nn.ReLU(),\n            *[hidden_block(h_channels) for _ in range(h_layers)],\n            nn.Linear(h_channels, out_channels) \n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n\nclass Unet(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 div_factor=8,\n                 prob_output=True,\n                 class_output=False\n\n    ):\n        super(Unet, self).__init__()\n\n        self.n_channels = in_channels\n        self.bilinear = True\n        self.sigmoid = nn.Sigmoid()\n        self.prob_output = prob_output\n        self.class_output = class_output\n\n        def double_conv(in_channels, out_channels):\n            return nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True),\n            )\n\n        def down(in_channels, out_channels):\n            return nn.Sequential(\n                nn.MaxPool2d(2),\n                double_conv(in_channels, out_channels)\n            )\n\n        class up(nn.Module):\n            def __init__(self, in_channels, out_channels, bilinear=True):\n                super().__init__()\n\n                if bilinear:\n                    self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n                else:\n                    self.up = nn.ConvTranpose2d(in_channels // 2, in_channels // 2,\n                                                kernel_size=2, stride=2)\n\n                self.conv = double_conv(in_channels, out_channels)\n\n            def forward(self, x1, x2):\n                x1 = self.up(x1)\n                # [?, C, H, W]\n                diffY = x2.size()[2] - x1.size()[2]\n                diffX = x2.size()[3] - x1.size()[3]\n\n                x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                diffY // 2, diffY - diffY // 2])\n                x = torch.cat([x2, x1], dim=1) ## why 1?\n                return self.conv(x)\n\n        self.inc = double_conv(self.n_channels, 64//div_factor)\n        self.down1 = down(64//div_factor, 128//div_factor)\n        self.down2 = down(128//div_factor, 256//div_factor)\n        self.down3 = down(256//div_factor, 512//div_factor)\n        self.down4 = down(512//div_factor, 512//div_factor)\n        self.up1 = up(1024//div_factor, 256//div_factor)\n        self.up2 = up(512//div_factor, 128//div_factor)\n        self.up3 = up(256//div_factor, 64//div_factor)\n        self.up4 = up(128//div_factor, 128//div_factor)\n        self.out = nn.Conv2d(128//div_factor, 1, kernel_size=1)\n\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n\n        #return self.out(x).permute(0,2,3,1)\n\n        if self.prob_output:\n            x = self.out(x)\n            return self.sigmoid(x).permute(0,2,3,1)\n        else:\n            return self.out(x).permute(0,2,3,1)","metadata":{"papermill":{"duration":0.032909,"end_time":"2024-06-12T12:54:19.395778","exception":false,"start_time":"2024-06-12T12:54:19.362869","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-09T21:12:36.465006Z","iopub.execute_input":"2024-09-09T21:12:36.465441Z","iopub.status.idle":"2024-09-09T21:12:36.487254Z","shell.execute_reply.started":"2024-09-09T21:12:36.465406Z","shell.execute_reply":"2024-09-09T21:12:36.486051Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass conv_block(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(conv_block, self).__init__()\n\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\nclass up_conv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(up_conv, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.up(x)\n        return x\n\nclass Attention_block(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super(Attention_block, self).__init__()\n\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, g, x):\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        out = x * psi\n        return out\n\nclass UNet_Attention(nn.Module):\n    def __init__(self, img_ch=12, output_ch=1):\n        super(UNet_Attention, self).__init__()\n\n        n1 = 64\n        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n\n        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.Conv1 = conv_block(img_ch, filters[0])\n        self.Conv2 = conv_block(filters[0], filters[1])\n        self.Conv3 = conv_block(filters[1], filters[2])\n        self.Conv4 = conv_block(filters[2], filters[3])\n        self.Conv5 = conv_block(filters[3], filters[4])\n\n        self.Up5 = up_conv(filters[4], filters[3])\n        self.Att5 = Attention_block(F_g=filters[3], F_l=filters[3], F_int=filters[2])\n        self.Up_conv5 = conv_block(filters[4], filters[3])\n\n        self.Up4 = up_conv(filters[3], filters[2])\n        self.Att4 = Attention_block(F_g=filters[2], F_l=filters[2], F_int=filters[1])\n        self.Up_conv4 = conv_block(filters[3], filters[2])\n\n        self.Up3 = up_conv(filters[2], filters[1])\n        self.Att3 = Attention_block(F_g=filters[1], F_l=filters[1], F_int=filters[0])\n        self.Up_conv3 = conv_block(filters[2], filters[1])\n\n        self.Up2 = up_conv(filters[1], filters[0])\n        self.Att2 = Attention_block(F_g=filters[0], F_l=filters[0], F_int=32)\n        self.Up_conv2 = conv_block(filters[1], filters[0])\n\n        self.Conv = nn.Conv2d(filters[0], output_ch, kernel_size=1, stride=1, padding=0)\n\n\n    def forward(self, x):\n\n        e1 = self.Conv1(x)\n\n        e2 = self.Maxpool1(e1)\n        e2 = self.Conv2(e2)\n\n        e3 = self.Maxpool2(e2)\n        e3 = self.Conv3(e3)\n\n        e4 = self.Maxpool3(e3)\n        e4 = self.Conv4(e4)\n\n        e5 = self.Maxpool4(e4)\n        e5 = self.Conv5(e5)\n\n        d5 = self.Up5(e5)\n\n        x4 = self.Att5(g=d5, x=e4)\n        d5 = torch.cat((x4, d5), dim=1)\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n        x3 = self.Att4(g=d4, x=e3)\n        d4 = torch.cat((x3, d4), dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        x2 = self.Att3(g=d3, x=e2)\n        d3 = torch.cat((x2, d3), dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        x1 = self.Att2(g=d2, x=e1)\n        d2 = torch.cat((x1, d2), dim=1)\n        d2 = self.Up_conv2(d2)\n\n        out = self.Conv(d2)\n\n        return out\n\n# Example usage:\n\n# Instantiate the model\nmodel = UNet_Attention(img_ch=12, output_ch=1)\n\n# Example input tensor with shape [batch_size, channels, height, width]\ninput_tensor = torch.randn(16, 12, 128, 128)  # Example batch size of 16\n\n# Example target tensor with shape [batch_size, height, width]\ntarget_tensor = torch.randn(16, 128, 128)\n\n# Adjust the target tensor to match the output shape\ntarget_tensor = target_tensor.unsqueeze(1)\n\n# Forward pass\noutput = model(input_tensor)\n\n# Check output shape\nprint(f\"Output shape: {output.shape}\")  # Should be [16, 1, 128, 128]\nprint(f\"Target shape: {target_tensor.shape}\")  # Should be [16, 1, 128, 128]","metadata":{"execution":{"iopub.status.busy":"2024-09-09T21:12:36.488496Z","iopub.execute_input":"2024-09-09T21:12:36.488841Z","iopub.status.idle":"2024-09-09T21:12:41.514594Z","shell.execute_reply.started":"2024-09-09T21:12:36.488797Z","shell.execute_reply":"2024-09-09T21:12:41.513634Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Output shape: torch.Size([16, 1, 128, 128])\nTarget shape: torch.Size([16, 1, 128, 128])\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n           \n        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n                               nn.ReLU(),\n                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T21:12:41.517180Z","iopub.execute_input":"2024-09-09T21:12:41.517491Z","iopub.status.idle":"2024-09-09T21:12:41.529783Z","shell.execute_reply.started":"2024-09-09T21:12:41.517456Z","shell.execute_reply":"2024-09-09T21:12:41.528905Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass conv_block(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(conv_block, self).__init__()\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True))\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(out_ch))\n\n    def forward(self, x):\n        x = self.conv1(x) + self.conv2(x)\n        return x\n\nclass up_conv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(up_conv, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.up(x)\n        return x\n\nclass Attention_block(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super(Attention_block, self).__init__()\n\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, g, x):\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        out = x * psi\n        return out\n\nclass Res_UNet_Channel_Attention(nn.Module):\n    def __init__(self, img_ch=12, output_ch=1):\n        super(Res_UNet_Channel_Attention, self).__init__()\n\n        n1 = 64\n        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16, n1 * 32]\n\n        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.Conv1 = conv_block(img_ch, filters[0])\n        self.Conv2 = conv_block(filters[0], filters[1])\n        self.Conv3 = conv_block(filters[1], filters[2])\n        self.Conv4 = conv_block(filters[2], filters[3])\n        self.Conv5 = conv_block(filters[3], filters[4])\n        self.conv6 = conv_block(filters[4], filters[5])\n\n        self.Up6 = up_conv(filters[5], filters[4])\n        self.Att6 = Attention_block(F_g=filters[4], F_l=filters[4], F_int=filters[3])\n        self.Up_conv6 = conv_block(filters[5], filters[4])\n\n        self.Up5 = up_conv(filters[4], filters[3])\n        self.Att5 = Attention_block(F_g=filters[3], F_l=filters[3], F_int=filters[2])\n        self.Up_conv5 = conv_block(filters[4], filters[3])\n\n        self.Up4 = up_conv(filters[3], filters[2])\n        self.Att4 = Attention_block(F_g=filters[2], F_l=filters[2], F_int=filters[1])\n        self.Up_conv4 = conv_block(filters[3], filters[2])\n\n        self.Up3 = up_conv(filters[2], filters[1])\n        self.Att3 = Attention_block(F_g=filters[1], F_l=filters[1], F_int=filters[0])\n        self.Up_conv3 = conv_block(filters[2], filters[1])\n\n        self.Up2 = up_conv(filters[1], filters[0])\n        self.Att2 = Attention_block(F_g=filters[0], F_l=filters[0], F_int=32)\n        self.Up_conv2 = conv_block(filters[1], filters[0])\n\n        self.Conv = nn.Conv2d(filters[0], output_ch, kernel_size=1, stride=1, padding=0)\n        \n        self.Ch_att1 = ChannelAttention(in_planes=filters[0], ratio=8)\n        self.Ch_att2 = ChannelAttention(in_planes=filters[1], ratio=8)\n        self.Ch_att3 = ChannelAttention(in_planes=filters[2], ratio=8)\n        self.Ch_att4 = ChannelAttention(in_planes=filters[3], ratio=8)\n        self.Ch_att5 = ChannelAttention(in_planes=filters[4], ratio=8)\n\n\n    def forward(self, x):\n\n        e1 = self.Conv1(x)\n\n        e2 = self.Maxpool1(e1)\n        e2 = self.Conv2(e2)\n\n        e3 = self.Maxpool2(e2)\n        e3 = self.Conv3(e3)\n\n        e4 = self.Maxpool3(e3)\n        e4 = self.Conv4(e4)\n\n        e5 = self.Maxpool4(e4)\n        e5 = self.Conv5(e5)\n\n        e6 = self.Maxpool5(e5)\n        e6 = self.conv6(e6)\n\n        d6 = self.Up6(e6)\n\n#         x5 = self.Att6(g=d6, x=self.Ch_att5(e5))\n        x5 = self.Att6(g=d6, x=e5)\n        d6 = torch.cat((x5, d6), dim=1)\n        d6 = self.Up_conv6(d6)\n\n        d5 = self.Up5(d6)\n#         x4 = self.Att5(g=d5, x=self.Ch_att4(e4))\n        x4 = self.Att5(g=d5, x=e4)\n        d5 = torch.cat((x4, d5), dim=1)\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n#         x3 = self.Att4(g=d4, x=self.Ch_att3(e3))\n        x3 = self.Att4(g=d4, x=e3)\n        d4 = torch.cat((x3, d4), dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n#         x2 = self.Att3(g=d3, x=self.Ch_att2(e2))\n        x2 = self.Att3(g=d3, x=e2)\n        d3 = torch.cat((x2, d3), dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n#         x1 = self.Att2(g=d2, x=self.Ch_att1(e1))\n        x1 = self.Att2(g=d2, x=e1)\n        d2 = torch.cat((x1, d2), dim=1)\n        d2 = self.Up_conv2(d2)\n\n        out = self.Conv(d2)\n\n        return out\n\n# Example usage:\n\n# Instantiate the model\nmodel = Res_UNet_Channel_Attention(img_ch=12, output_ch=1)\n\n# Example input tensor with shape [batch_size, channels, height, width]\ninput_tensor = torch.randn(16, 12, 128, 128)  # Example batch size of 16\n\n# Example target tensor with shape [batch_size, height, width]\ntarget_tensor = torch.randn(16, 128, 128)\n\n# Adjust the target tensor to match the output shape\ntarget_tensor = target_tensor.unsqueeze(1)\n\n# Forward pass\noutput = model(input_tensor)\n\n# Check output shape\nprint(f\"Output shape: {output.shape}\")  # Should be [16, 1, 128, 128]\nprint(f\"Target shape: {target_tensor.shape}\")  # Should be [16, 1, 128, 128]","metadata":{"execution":{"iopub.status.busy":"2024-09-09T21:12:41.531253Z","iopub.execute_input":"2024-09-09T21:12:41.531552Z","iopub.status.idle":"2024-09-09T21:12:48.464522Z","shell.execute_reply.started":"2024-09-09T21:12:41.531519Z","shell.execute_reply":"2024-09-09T21:12:48.463525Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Output shape: torch.Size([16, 1, 128, 128])\nTarget shape: torch.Size([16, 1, 128, 128])\n","output_type":"stream"}]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\n\nclass Trainer():\n    \"\"\"\n    Training class for the neural process models\n    \"\"\"\n\n    def __init__(self,\n                 model,\n                 train_loader,\n                 val_loader,\n                 train_dataset,\n                 loss_function,\n                 save_path,\n                 learning_rate):\n\n        # Model and data\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.train_dataset = train_dataset\n        self.save_path = save_path\n\n        # Training parameters\n        self.opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n        self.loss_function = loss_function\n        self.sched = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            self.opt, mode='min', factor=0.5, patience=7, verbose=True)\n\n        # Losses\n        self.losses = []\n        self.mIoUs = []\n        self.losses_return = []\n        self.saved_loss = 0\n\n    def plot_losses(self):\n        \"\"\"\n        Plot losses and IoUs in same figure\n        \"\"\"\n        fig, ax1 = plt.subplots()\n\n        color = 'tab:red'\n        ax1.set_xlabel('Epoch')\n        ax1.set_ylabel('Loss', color=color)\n        ax1.plot(self.losses, color=color)\n        ax1.tick_params(axis='y', labelcolor=color)\n\n        ax2 = ax1.twinx()\n        color = 'tab:blue'\n        ax2.set_ylabel('mIoU', color=color)\n        ax2.plot(self.mIoUs, color=color)\n        ax2.tick_params(axis='y', labelcolor=color)\n\n        fig.tight_layout()\n        plt.show()\n\n    def _unravel_to_numpy(self, x):\n        return x.view(-1).detach().cpu().numpy()\n\n    def eval_epoch(self, verbose=False):\n        self.model.eval()\n        lf = []\n        ious = []\n\n        outs = []\n        ts = []\n\n        def calculate_iou(pred, target):\n            eps = 1e-6\n            # convert to sigmoid then to binary\n            pred = torch.sigmoid(pred)\n            pred = (pred > 0.5).float()\n            target = target.float()\n            \n            return (torch.sum(pred * target) + eps) / (torch.sum(pred + target) + eps)\n        \n        with torch.no_grad():\n            for task in self.val_loader:\n                out = self.model(task[\"pred\"])\n                lf.append(self.loss_function(torch.squeeze(out), task[\"target\"]))\n                ious.append(calculate_iou(out, task[\"target\"]))\n                outs.append(out.detach().cpu().numpy())\n                ts.append(task[\"target\"].detach().cpu().numpy())\n\n        # Get loss function\n        log_loss = torch.mean(torch.stack(lf))\n        print(\"- Log loss: {}\".format(log_loss.item()))\n        mIoU = torch.mean(torch.tensor(ious))\n        print(\"- mIoU: {}\".format(mIoU.item()))\n\n        if verbose:\n            return log_loss, np.concatenate(outs, axis=0), np.concatenate(ts, axis=0)\n\n        return log_loss, mIoU\n\n    def train(self, n_epochs=100):\n        best_loss = float('inf')\n        total_loss_list = []\n\n        for epoch in range(n_epochs):\n            print(f\"{epoch} epoch is running.....\")\n            self.model.train()\n            epoch_loss = 0.0\n            \n            for task in self.train_loader:\n                self.opt.zero_grad()\n                out = self.model(task[\"pred\"])\n                loss = self.loss_function(torch.squeeze(out), task[\"target\"])\n                \n                loss.backward()\n                \n                self.opt.step()\n                self.opt.zero_grad()\n                out = self.model(task[\"pred\"])\n                losss = self.loss_function(torch.squeeze(out), task[\"target\"])\n                self.saved_loss = self.saved_loss + losss\n                epoch_loss += loss.item()\n\n            avg_loss = epoch_loss / len(self.train_loader)\n            total_loss_list.append(avg_loss)\n            \n            # Evaluate on validation set\n            val_loss, mIoU = self.eval_epoch()\n            self.sched.step(val_loss)\n            \n            if val_loss < best_loss:\n                best_loss = val_loss\n                torch.save(self.model.state_dict(), f\"{self.save_path}best_model.pth\")\n\n            # Store losses and mIoU for plotting\n            self.losses.append(avg_loss)\n            self.mIoUs.append(mIoU.item())\n\n        # Calculate the average loss over all epochs\n        avg_loss_over_epochs = sum(total_loss_list) / len(total_loss_list)\n        \n        return self.saved_loss\n","metadata":{"papermill":{"duration":0.026808,"end_time":"2024-06-12T12:54:19.427168","exception":false,"start_time":"2024-06-12T12:54:19.40036","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-09T21:12:48.466016Z","iopub.execute_input":"2024-09-09T21:12:48.466591Z","iopub.status.idle":"2024-09-09T21:12:48.490619Z","shell.execute_reply.started":"2024-09-09T21:12:48.466547Z","shell.execute_reply":"2024-09-09T21:12:48.489510Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# ","metadata":{"execution":{"iopub.status.busy":"2024-09-09T21:12:48.492140Z","iopub.execute_input":"2024-09-09T21:12:48.492645Z","iopub.status.idle":"2024-09-09T21:12:48.503362Z","shell.execute_reply.started":"2024-09-09T21:12:48.492605Z","shell.execute_reply":"2024-09-09T21:12:48.502536Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /kaggle/working/train_out\n!mkdir -p /kaggle/working/eval_out","metadata":{"papermill":{"duration":1.939715,"end_time":"2024-06-12T12:54:21.37123","exception":false,"start_time":"2024-06-12T12:54:19.431515","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-09T21:12:48.504519Z","iopub.execute_input":"2024-09-09T21:12:48.505221Z","iopub.status.idle":"2024-09-09T21:12:50.663268Z","shell.execute_reply.started":"2024-09-09T21:12:48.505183Z","shell.execute_reply":"2024-09-09T21:12:50.661833Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!pip install monai","metadata":{"execution":{"iopub.status.busy":"2024-09-09T21:12:50.664867Z","iopub.execute_input":"2024-09-09T21:12:50.665209Z","iopub.status.idle":"2024-09-09T21:13:06.056719Z","shell.execute_reply.started":"2024-09-09T21:12:50.665169Z","shell.execute_reply":"2024-09-09T21:13:06.055470Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Collecting monai\n  Downloading monai-1.3.2-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.10/site-packages (from monai) (2.4.0)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from monai) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9->monai) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9->monai) (1.3.0)\nDownloading monai-1.3.2-py3-none-any.whl (1.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: monai\nSuccessfully installed monai-1.3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.ops import sigmoid_focal_loss\nfrom monai.losses.hausdorff_loss import HausdorffDTLoss\n\ndef loss(pred, target):\n#     print(pred.shape, target.shape)\n    bce_loss = nn.BCEWithLogitsLoss(reduction=\"none\") \n    ll = bce_loss(pred, target)\n\n    ll = ll.sum(dim=(-2,-1)) #*mask\n    return ll.mean()\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.75, gamma=2.0, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n        self.bce_loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n    \n    def forward(self, pred, target):\n        loss = self.bce_loss(pred, target)\n        prob = torch.sigmoid(pred)  # Predicted probability\n        alpha = torch.where(target == 1, self.alpha, 1 - self.alpha)  # Class balancing factor\n        focal_weight = torch.where(target == 1, 1 - prob, prob)  # Focusing weight\n        focal_weight = alpha * focal_weight**self.gamma  # Apply alpha and gamma\n        focal_loss = focal_weight * loss\n        \n        focal_loss = focal_loss.sum(dim=(-2,-1)) #*mask\n        \n        return focal_loss.mean() if self.reduction == 'mean' else focal_loss\n\n    \nclass MultiScalePoolingLoss(nn.Module):\n    def __init__(self, alpha=0.75, gamma=2, itr=3, ratio=None):\n        super(MultiScalePoolingLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.itr = itr\n        self.focal_loss = FocalLoss(alpha=alpha, gamma=gamma)\n        self.ratio = torch.tensor(ratio)\n    \n    def forward(self, pred, target):\n        losses = torch.zeros(self.itr)\n        if(pred.dim() == 2):\n            pred = pred.unsqueeze(0)\n            target = target.unsqueeze(0)\n        elif(pred.dim() == 3):\n            pred = pred.unsqueeze(1)\n            target = target.unsqueeze(1)\n        for i in range(self.itr):\n            losses[i] = self.focal_loss(pred, target)\n            pred = F.max_pool2d(pred, kernel_size=2, stride=2)\n            target = F.max_pool2d(target, kernel_size=2, stride=2)\n        \n        if self.ratio is not None:\n            losses = losses * self.ratio\n        else:\n            losses = losses * 1/self.itr\n        \n        return torch.sum(losses)\n\n    \nclass DiceBCELoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceBCELoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        # Comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)\n\n        # Flatten label and prediction tensors\n        inputs = inputs.view(inputs.size(0), -1)\n        targets = targets.view(targets.size(0), -1)\n        \n        # Compute Dice loss\n        intersection = (inputs * targets).sum(dim=1)\n        dice_loss = 1 - (2.0 * intersection + smooth) / (inputs.sum(dim=1) + targets.sum(dim=1) + smooth)\n\n        # Compute BCE loss\n        BCE = F.binary_cross_entropy(inputs, targets, reduction='none')\n        BCE = BCE.mean(dim=1)\n\n        # Combine Dice and BCE losses\n        Dice_BCE = BCE + dice_loss\n\n        return Dice_BCE.mean()\n\n    \nclass HausdorffDT_Loss(nn.Module):\n    def __init__(self):\n        super(HausdorffDT_Loss, self).__init__()\n        self.hd_loss = HausdorffDTLoss(reduction='mean', sigmoid=True)\n        self.focal_loss = FocalLoss\n\n    def forward(self, inputs, targets):\n        # Comment out if your model contains a sigmoid or equivalent activation layer\n        return self.hd_loss(inputs.unsqueeze(1), targets.unsqueeze(1))\n\n    \n# class HausdorffDT_Focal_Loss(nn.Module):\n#     def __init__(self, focal_weight = 0.65):\n#         super(HausdorffDT_Focal_Loss, self).__init__()\n#         self.hd_loss = HausdorffDTLoss(reduction='none', sigmoid=True)\n#         self.focal_loss = FocalLoss(alpha=0.75, gamma=2.0, reduction='none')\n#         self.focal_weight = focal_weight\n\n#     def forward(self, inputs, targets):\n#         # Comment out if your model contains a sigmoid or equivalent activation layer\n#         hl = self.hd_loss(inputs.unsqueeze(1), targets.unsqueeze(1))\n#         fl = self.focal_loss(inputs, targets)\n#         final_loss = self.focal_weight * torch.squeeze(fl) + (1-self.focal_weight) * torch.squeeze(hl)\n#         return torch.mean(final_loss)\n    \n\npred = torch.randn(16, 1, 128, 128).float()\ntarget = torch.randint(0, 2, (16, 1, 128, 128)).float()\n\n# focal_loss = FocalLoss(alpha=0.75, gamma=1.0)\n# hausdorffDTLoss = HausdorffDT_Loss(reduction='none', sigmoid=True)\n# dbceloss = DiceBCELoss()\n# multi_scale_pooling_loss = MultiScalePoolingLoss(alpha=0.75, gamma=2.0, itr=3, ratio=[0.5, 0.3, 0.2])\n# hausdorffDT_focal_loss = HausdorffDT_Focal_Loss(focal_weight = 0.65)\n\n# # focal_loss(pred, target)\n# # multi_scale_pooling_loss(pred, target)\n# print(hausdorffDT_focal_loss(pred, target))","metadata":{"execution":{"iopub.status.busy":"2024-09-09T21:13:06.061867Z","iopub.execute_input":"2024-09-09T21:13:06.062209Z","iopub.status.idle":"2024-09-09T21:13:46.698180Z","shell.execute_reply.started":"2024-09-09T21:13:06.062170Z","shell.execute_reply":"2024-09-09T21:13:46.697158Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n  from torch.distributed.optim import ZeroRedundancyOptimizer\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass HausdorffDT_Focal_Loss(nn.Module):\n    def __init__(self, focal_weight=0.65):\n        super(HausdorffDT_Focal_Loss, self).__init__()\n        self.hd_loss = HausdorffDTLoss(reduction='none', sigmoid=True)  # Assuming defined elsewhere\n        self.focal_loss = FocalLoss(alpha=0.75, gamma=2.0, reduction='none')  # Assuming defined elsewhere\n        self.focal_weight = nn.Parameter(torch.tensor(focal_weight, dtype=torch.float32))  # Learnable focal_weight\n\n    def forward(self, inputs, targets):\n        hl = self.hd_loss(inputs.unsqueeze(1), targets.unsqueeze(1))\n        fl = self.focal_loss(inputs, targets)\n        final_loss = self.focal_weight * torch.squeeze(fl) + (1 - self.focal_weight) * torch.squeeze(hl)\n        return torch.mean(final_loss)\n\nclass FocalWeightOptimizer:\n    def __init__(self, trainer, loss_function, initial_focal_weight=0.65, learning_rate=0.01):\n        self.trainer = trainer\n        self.loss_function = loss_function\n        self.focal_weight = loss_function.focal_weight  # Use the learnable focal_weight from the loss function\n        self.opt = optim.Adam([self.focal_weight], lr=learning_rate)  # Optimizer for focal_weight\n        self.best_focal_weight = initial_focal_weight\n        self.best_loss = float('inf')\n\n    def optimize_focal_weight(self, model, n_outer_epochs=5, n_inner_epochs=20, random_state=42):\n        torch.manual_seed(random_state)\n\n        print(f\"Initial focal weight: {self.focal_weight.item()}\")\n\n        for outer_epoch in range(n_outer_epochs):\n            model.apply(self.reset_model_weights)\n\n            print(f\"\\nOuter Epoch {outer_epoch+1}/{n_outer_epochs}\")\n            self.trainer.loss_function = self.loss_function  # Use updated loss function\n\n            # Train the model for n_inner_epochs\n            avg_loss = self.trainer.train(n_epochs=n_inner_epochs)\n\n            # Reset gradients and compute new loss\n            self.opt.zero_grad()\n\n            # Compute loss in training loop\n#             avg_loss_tensor = torch.tensor(avg_loss, dtype=torch.float32, requires_grad=True)\n            \n            # Backpropagate the loss\n            avg_loss.backward()\n            self.opt.step()  # Update focal_weight\n\n            print(f\"Focal weight: {self.focal_weight.item()}, Loss: {avg_loss}\")\n\n            if avg_loss < self.best_loss:\n                self.best_loss = avg_loss\n                self.best_focal_weight = self.focal_weight.item()\n\n        print(f\"\\nOptimization Complete! Best Focal Weight: {self.best_focal_weight}, Best Loss: {self.best_loss}\")\n\n    def reset_model_weights(self, m):\n        if isinstance(m, (nn.Linear, nn.Conv2d)):\n            m.reset_parameters()\n\n\n\n\n# Assuming you've already defined your model and datasets\n\n# # Instantiate your loss function with an initial focal weight\n# initial_focal_weight = 0.65\n# loss_function = HausdorffDT_Focal_Loss(focal_weight=initial_focal_weight)\n\n# # Initialize your trainer class\n# trainer = Trainer(\n#     model=model,\n#     train_loader=train_loader,\n#     val_loader=val_loader,\n#     train_dataset=train_dataset,\n#     loss_function=loss_function,\n#     save_path=save_path,\n#     learning_rate=0.001\n# )\n\n# # Instantiate the FocalWeightOptimizer with a trainer and an initial focal weight\n# focal_weight_optimizer = FocalWeightOptimizer(trainer, loss_function, initial_focal_weight=initial_focal_weight, learning_rate=0.01)\n\n# # Perform optimization of the focal weight over several outer epochs\n# focal_weight_optimizer.optimize_focal_weight(model=model, n_outer_epochs=5, n_inner_epochs=20, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T21:13:46.699613Z","iopub.execute_input":"2024-09-09T21:13:46.700319Z","iopub.status.idle":"2024-09-09T21:13:46.715650Z","shell.execute_reply.started":"2024-09-09T21:13:46.700281Z","shell.execute_reply":"2024-09-09T21:13:46.714707Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# from models import *\n# from trainer import *\n# from loader import *\nfrom torch.distributions.bernoulli import Bernoulli\nfrom torch.distributions.multivariate_normal import MultivariateNormal\nimport sys\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom torch.nn import DataParallel\n\n#python3 train.py 12 FINAL_12/\n\n# Input arguments\n# channels = int(sys.argv[1])\n# out_dir = sys.argv[2]\n\nchannels = 12\nout_dir = '/kaggle/working/train_out/'\n\n# Set up \ntorch.manual_seed(0)\ntorch.backends.cudnn.benchmark = True\ndevice = torch.device('cuda')\n\n# Set up model\nmodel = UNet_Attention(img_ch=channels,\n            output_ch=1,\n            )\n# model = Unet(in_channels=channels,\n#             out_channels=1,\n#             div_factor=1, \n#             prob_output=False)\n# model = Res_UNet_Channel_Attention(img_ch=12, output_ch=1)\nmodel = model.to(device)\nmodel = nn.DataParallel(model)\n\n# Set up loss function\n# loss_fn = MultiScalePoolingLoss(alpha=0.75, gamma=0.0, itr=4, ratio=[0.4, 0.2, 0.2, 0.3])\n# loss_fn = HausdorffDT_Focal_Loss(focal_weight = 0.65)\n\ntrain_dataset = MethaneLoader(device = \"cuda\", mode=\"train\", plume_id=None, channels=channels)\ntest_dataset = MethaneLoader(device = \"cuda\", mode=\"test\", plume_id=None, channels=channels)\n\n#print(train_dataset.__len__())\n\ntrain_loader = DataLoader(train_dataset, \n                          batch_size = 16, \n                          shuffle = True)\n\ntest_loader = DataLoader(test_dataset, \n                          batch_size = 16, \n                          shuffle = True)\n\n\n# Make the trainer\n# trainer = Trainer(model,\n#                   train_loader,\n#                   test_loader,\n#                   train_dataset,\n#                   loss_fn,\n#                   out_dir,\n#                   1e-4)\n\n\n# Instantiate your loss function with an initial focal weight\ninitial_focal_weight = 0.65\nloss_function = HausdorffDT_Focal_Loss(focal_weight=initial_focal_weight)\n\n# Initialize your trainer class\ntrainer = Trainer(\n    model=model,\n    train_loader=train_loader,\n    val_loader=test_loader,\n    train_dataset=train_dataset,\n    loss_function=loss_function,\n    save_path=out_dir,\n    learning_rate=1e-3\n)\n\n# Instantiate the FocalWeightOptimizer with a trainer and an initial focal weight\nfocal_weight_optimizer = FocalWeightOptimizer(trainer, loss_function, initial_focal_weight=initial_focal_weight, learning_rate=0.01)\n\n# Perform optimization of the focal weight over several outer epochs\nfocal_weight_optimizer.optimize_focal_weight(model=model, n_outer_epochs=5, n_inner_epochs=3, random_state=42)\n\n# Train\n# trainer.train(n_epochs=150)","metadata":{"papermill":{"duration":3349.124987,"end_time":"2024-06-12T13:50:10.500691","exception":false,"start_time":"2024-06-12T12:54:21.375704","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-09T21:13:46.716912Z","iopub.execute_input":"2024-09-09T21:13:46.717211Z","iopub.status.idle":"2024-09-09T21:14:01.853536Z","shell.execute_reply.started":"2024-09-09T21:13:46.717178Z","shell.execute_reply":"2024-09-09T21:14:01.851973Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Initial focal weight: 0.6499999761581421\n\nOuter Epoch 1/5\n0 epoch is running.....\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/monai/losses/hausdorff_loss.py:171: UserWarning: single channel prediction, `include_background=False` ignored.\n  warnings.warn(\"single channel prediction, `include_background=False` ignored.\")\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m focal_weight_optimizer \u001b[38;5;241m=\u001b[39m FocalWeightOptimizer(trainer, loss_function, initial_focal_weight\u001b[38;5;241m=\u001b[39minitial_focal_weight, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Perform optimization of the focal weight over several outer epochs\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[43mfocal_weight_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_focal_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_outer_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_inner_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# trainer.train(n_epochs=150)\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 39\u001b[0m, in \u001b[0;36mFocalWeightOptimizer.optimize_focal_weight\u001b[0;34m(self, model, n_outer_epochs, n_inner_epochs, random_state)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mloss_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function  \u001b[38;5;66;03m# Use updated loss function\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Train the model for n_inner_epochs\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_inner_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Reset gradients and compute new loss\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n","Cell \u001b[0;32mIn[6], line 112\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 112\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(torch\u001b[38;5;241m.\u001b[39msqueeze(out), task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    115\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:184\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[3], line 120\u001b[0m, in \u001b[0;36mUNet_Attention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m d5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUp5(e5)\n\u001b[1;32m    119\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAtt5(g\u001b[38;5;241m=\u001b[39md5, x\u001b[38;5;241m=\u001b[39me4)\n\u001b[0;32m--> 120\u001b[0m d5 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md5\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m d5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUp_conv5(d5)\n\u001b[1;32m    123\u001b[0m d4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUp4(d5)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 25.12 MiB is free. Process 7184 has 15.86 GiB memory in use. Of the allocated memory 15.50 GiB is allocated by PyTorch, and 78.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 25.12 MiB is free. Process 7184 has 15.86 GiB memory in use. Of the allocated memory 15.50 GiB is allocated by PyTorch, and 78.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}]},{"cell_type":"code","source":"### for clearing directory\n### skip it\n#!rm -rf /kaggle/working/*","metadata":{"papermill":{"duration":1.833333,"end_time":"2024-06-12T13:50:14.203088","exception":false,"start_time":"2024-06-12T13:50:12.369755","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-09T21:14:01.854597Z","iopub.status.idle":"2024-09-09T21:14:01.855085Z","shell.execute_reply.started":"2024-09-09T21:14:01.854847Z","shell.execute_reply":"2024-09-09T21:14:01.854875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize(idx):\n    file_paths = glob('/kaggle/input/ch4net-dataset/data/val/mbmp/*/{}.npy'.format(idx))\n    for file in file_paths:\n        category = file.split(\"/\")[-2]\n        print(category)\n        image1 = np.load(file)\n    image2 = np.load(\"/kaggle/working/eval_out/target_{}.npy\".format(idx))\n    image3 = np.load(\"/kaggle/working/eval_out/out_{}.npy\".format(idx))\n\n    # Squeeze the extra dimension if needed\n    image1 = np.squeeze(image1)\n    image2 = np.squeeze(image2)\n    image3 = np.squeeze(image3)\n\n    print(image1.shape, image2.shape, image3.shape)\n\n    # Create a figure with 3 subplots side by side\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    # Display each image\n    axes[0].imshow(image1, cmap='magma')\n    axes[0].set_title('MBMP')\n    axes[0].axis('off')  # Hide axis\n\n    axes[1].imshow(image2, cmap='magma')\n    axes[1].set_title('Target')\n    axes[1].axis('off')  # Hide axis\n\n    axes[2].imshow(image3, cmap='magma')\n    axes[2].set_title('Out')\n    axes[2].axis('off')  # Hide axis\n\n    # Show the figure\n    plt.tight_layout()\n    plt.show()","metadata":{"papermill":{"duration":1.882555,"end_time":"2024-06-12T13:50:17.953892","exception":false,"start_time":"2024-06-12T13:50:16.071337","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-09T21:14:01.857172Z","iopub.status.idle":"2024-09-09T21:14:01.857675Z","shell.execute_reply.started":"2024-09-09T21:14:01.857408Z","shell.execute_reply":"2024-09-09T21:14:01.857433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\n\n# from models import *\n# from trainer import *\n# from loader import *\n\nfrom torch.utils.data import DataLoader\nfrom glob import glob\n\n# python3 gen_eval_preds.py 12 final_12_all/ final_12_preds/ 0\n# in_dir = sys.argv[2]\n# out_dir = sys.argv[3]\n# channels = int(sys.argv[1])\n# alli_yn = bool(int(sys.argv[4]))\n\nin_dir = \"/kaggle/working/train_out/\"\nout_dir = \"/kaggle/working/eval_out/\"\nchannels = 12\nalli_yn = False\n\nprint(\"Loading losses...\")\nlosses = np.load(in_dir+\"losses.npy\")\nbest_epoch = np.argmin(losses)\nprint('best epoch: ', best_epoch)\n\ndevice = torch.device('cuda')\n\n# Set up model\nmodel = UNet_Attention(img_ch=channels,\n            output_ch=1,\n            )\n# model = Unet(in_channels=channels,\n#             out_channels=1,\n#             div_factor=1, \n#             prob_output=False)\n# model = Res_UNet_Channel_Attention(img_ch=12, output_ch=1)\nmodel = model.to(device)\nmodel = nn.DataParallel(model)\n\nmodel.load_state_dict(torch.load(in_dir+\"final_model\",\n                      map_location=torch.device('cuda'))[\"model_state_dict\"])\n# model.load_state_dict(torch.load('/kaggle/input/ch4net_attention/pytorch/default/1/final_model',\n#                       map_location=torch.device('cuda'))[\"model_state_dict\"])\nmodel.eval()\nprint()","metadata":{"papermill":{"duration":17.430864,"end_time":"2024-06-12T13:50:37.283462","exception":false,"start_time":"2024-06-12T13:50:19.852598","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-09T21:14:01.859424Z","iopub.status.idle":"2024-09-09T21:14:01.859950Z","shell.execute_reply.started":"2024-09-09T21:14:01.859667Z","shell.execute_reply":"2024-09-09T21:14:01.859694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef calculate_iou(pred, target):\n    \"\"\"\n    Calculate Intersection over Union (IoU)\n    \"\"\"\n    pred = pred > 0  # Convert to binary mask\n    target = target > 0  # Convert to binary mask\n    intersection = np.logical_and(target, pred)\n    union = np.logical_or(target, pred)\n    iou_score = np.sum(intersection) / np.sum(union)\n    return iou_score\n\ndef calculate_accuracy(pred, target):\n    \"\"\"\n    Calculate pixel-wise accuracy\n    \"\"\"\n    pred = pred > 0  # Convert to binary mask\n    target = target > 0  # Convert to binary mask\n    correct = np.sum(pred == target)\n    total = pred.size\n    return correct / total","metadata":{"execution":{"iopub.status.busy":"2024-09-09T21:14:01.861531Z","iopub.status.idle":"2024-09-09T21:14:01.862026Z","shell.execute_reply.started":"2024-09-09T21:14:01.861756Z","shell.execute_reply":"2024-09-09T21:14:01.861796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom sklearn.metrics import balanced_accuracy_score\n\ndef calculate_metrics(pred, target, epsilon=1e-6):\n    \"\"\"\n    Calculate IoU, Accuracy, Recall, Balanced Accuracy, False Positive Rate, and False Negative Rate\n    \"\"\"\n    pred = F.sigmoid(torch.tensor(pred)).numpy()\n    pred = pred > 0.5 # Convert to binary mask\n    target = target > 0  # Convert to binary mask\n    \n    true_positive = np.sum(np.logical_and(pred, target))\n    false_positive = np.sum(np.logical_and(pred, np.logical_not(target)))\n    false_negative = np.sum(np.logical_and(np.logical_not(pred), target))\n    true_negative = np.sum(np.logical_and(np.logical_not(pred), np.logical_not(target)))\n    \n    # IoU\n    union = np.sum(np.logical_or(pred, target))\n    iou = (true_positive + epsilon) / (union + epsilon)\n    \n    # Accuracy\n    total = pred.size\n    accuracy = (true_positive + true_negative) / total\n    \n    # Recall (True Positive Rate)\n    recall = (true_positive + epsilon) / (true_positive + false_negative + epsilon)\n    \n    # False Positive Rate\n    fpr = (false_positive + epsilon) / (false_positive + true_negative + epsilon)\n    \n    # False Negative Rate\n    fnr = (false_negative + epsilon) / (false_negative + true_positive + epsilon)\n    \n    # Balanced Accuracy\n#     balanced_acc = balanced_accuracy_score(target.flatten(), pred.flatten())\n    balanced_acc = None\n    \n    return iou, accuracy, recall, balanced_acc, fpr, fnr\n\n# ... [Previous imports and setup code remains the same]\n\nlosses = []\nious = []\naccuracies = []\nrecalls = []\nbalanced_accs = []\nfprs = []\nfnrs = []\n\n# Iterate over each plume\nfor i in tqdm(range(250)):\n    preds = []\n    targets = []\n\n    test_dataset = MethaneLoader(\n        device=\"cuda\", mode=\"val\", alli=alli_yn, plume_id=i, channels=channels)\n\n    val_loader = DataLoader(test_dataset,\n                            batch_size=64,\n                            shuffle=False)\n\n    for batch in val_loader:\n        out = model(batch[\"pred\"])\n        preds.append(np.squeeze(out).cpu())\n        targets.append(np.squeeze(batch[\"target\"]).cpu())\n        loss_val = loss_fn(np.squeeze(out).unsqueeze(0), np.squeeze(batch[\"target\"]).unsqueeze(0))\n        losses.append(loss_val.item())\n        \n    if len(preds) > 0:\n        preds = torch.concat(preds, dim=0).detach().numpy()\n        targets = torch.concat(targets, dim=0).detach().numpy()\n\n        # Calculate metrics for this plume\n        iou, accuracy, recall, balanced_acc, fpr, fnr = calculate_metrics(preds, targets)\n        \n        ious.append(iou)\n        accuracies.append(accuracy)\n        recalls.append(recall)\n        balanced_accs.append(balanced_acc)\n        fprs.append(fpr)\n        fnrs.append(fnr)\n        \n        preds = F.sigmoid(torch.tensor(preds)).numpy()\n        preds = preds > 0.5 # Convert to binary mask\n        np.save(out_dir+f\"out_{i}.npy\", preds)\n        np.save(out_dir+f\"target_{i}.npy\", targets)\n\n# Calculate and print overall metrics\nmean_loss = np.mean(losses)\nmean_iou = np.mean(ious)\nmean_accuracy = np.mean(accuracies)\nmean_recall = np.mean(recalls)\n# mean_balanced_acc = np.mean(balanced_accs)\nmean_fpr = np.mean(fprs)\nmean_fnr = np.mean(fnrs)\n\nprint(f\"Mean Loss: {mean_loss:.4f}\")\nprint(f\"Mean IoU: {mean_iou:.4f}\")\nprint(f\"Mean Accuracy: {mean_accuracy:.4f}\")\nprint(f\"Mean Recall: {mean_recall:.4f}\")\n# print(f\"Mean Balanced Accuracy: {mean_balanced_acc:.4f}\")\nprint(f\"Mean False Positive Rate: {mean_fpr:.4f}\")\nprint(f\"Mean False Negative Rate: {mean_fnr:.4f}\")\n\n# Save metrics\nnp.save(out_dir+\"metrics.npy\", {\n    \"loss\": mean_loss,\n    \"iou\": mean_iou,\n    \"accuracy\": mean_accuracy,\n    \"recall\": mean_recall,\n#     \"balanced_accuracy\": mean_balanced_acc,\n    \"false_positive_rate\": mean_fpr,\n    \"false_negative_rate\": mean_fnr\n})","metadata":{"execution":{"iopub.status.busy":"2024-09-09T21:14:01.863927Z","iopub.status.idle":"2024-09-09T21:14:01.864403Z","shell.execute_reply.started":"2024-09-09T21:14:01.864150Z","shell.execute_reply":"2024-09-09T21:14:01.864175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from glob import glob\n\n# visualize masks\nsamples = 250\n\n# def visualize(idx):\n    \nfor i in range(samples):\n    idx = np.random.randint(0, samples)\n#     idx = i\n\n    file_paths = glob('/kaggle/input/ch4net-dataset/data/val/mbmp/*/{}.npy'.format(idx))\n    for file in file_paths:\n        category = file.split(\"/\")[-2]\n        print(category)\n        image1 = np.load(file)\n    image2 = np.load(\"/kaggle/working/eval_out/target_{}.npy\".format(idx))\n    image3 = np.load(\"/kaggle/working/eval_out/out_{}.npy\".format(idx))\n\n    # Squeeze the extra dimension if needed\n    image1 = np.squeeze(image1)\n    image2 = np.squeeze(image2)\n    image3 = np.squeeze(image3)\n#     print(image2.shape)\n    \n    eps = 1e-6\n    pred = image3\n    target = image2\n    iou = (np.sum(np.logical_and(pred, target)) + eps) / (np.sum(np.logical_or(pred, target)) + eps) * 100\n    \n    print(iou)\n\n#     print(image1.shape, image2.shape, image3.shape)\n#     print(losses[idx])\n\n    # Create a figure with 3 subplots side by side\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    # Display each image\n    axes[0].imshow(image1, cmap='magma')\n    axes[0].set_title('MBMP')\n    axes[0].axis('off')  # Hide axis\n\n    axes[1].imshow(image2, cmap='magma')\n    axes[1].set_title('Target')\n    axes[1].axis('off')  # Hide axis\n\n    axes[2].imshow(image3, cmap='magma')\n    axes[2].set_title('Out')\n    axes[2].axis('off')  # Hide axis\n\n    # Show the figure\n    plt.tight_layout()\n    plt.show()\n    \n    i = i + 1\n    if(i==20): break\n","metadata":{"papermill":{"duration":53.805544,"end_time":"2024-06-12T13:51:32.980913","exception":true,"start_time":"2024-06-12T13:50:39.175369","status":"failed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-09T21:14:01.865652Z","iopub.status.idle":"2024-09-09T21:14:01.866144Z","shell.execute_reply.started":"2024-09-09T21:14:01.865891Z","shell.execute_reply":"2024-09-09T21:14:01.865915Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
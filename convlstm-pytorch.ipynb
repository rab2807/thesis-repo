{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8905724,"sourceType":"datasetVersion","datasetId":5354472},{"sourceId":8967948,"sourceType":"datasetVersion","datasetId":5398602}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nimport numpy as np\n\nimport io\nimport imageio\nfrom ipywidgets import widgets, HBox\n\n# Use GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:51:46.107485Z","iopub.execute_input":"2024-07-17T09:51:46.107740Z","iopub.status.idle":"2024-07-17T09:51:50.541671Z","shell.execute_reply.started":"2024-07-17T09:51:46.107716Z","shell.execute_reply":"2024-07-17T09:51:50.540683Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Original ConvLSTM cell as proposed by Shi et al.\nclass ConvLSTMCell(nn.Module):\n\n    def __init__(self, in_channels, out_channels, \n    kernel_size, padding, activation, frame_size):\n\n        super(ConvLSTMCell, self).__init__()  \n\n        if activation == \"tanh\":\n            self.activation = torch.tanh \n        elif activation == \"relu\":\n            self.activation = torch.relu\n        \n        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n        self.conv = nn.Conv2d(\n            in_channels=in_channels + out_channels, \n            out_channels=4 * out_channels, \n            kernel_size=kernel_size, \n            padding=padding)           \n\n        # Initialize weights for Hadamard Products\n        self.W_ci = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n        self.W_co = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n        self.W_cf = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n\n    def forward(self, X, H_prev, C_prev):\n\n        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n        conv_output = self.conv(torch.cat([X, H_prev], dim=1))\n\n        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n        i_conv, f_conv, C_conv, o_conv = torch.chunk(conv_output, chunks=4, dim=1)\n\n        input_gate = torch.sigmoid(i_conv + self.W_ci * C_prev )\n        forget_gate = torch.sigmoid(f_conv + self.W_cf * C_prev )\n\n        # Current Cell output\n        C = forget_gate*C_prev + input_gate * self.activation(C_conv)\n\n        output_gate = torch.sigmoid(o_conv + self.W_co * C )\n\n        # Current Hidden State\n        H = output_gate * self.activation(C)\n\n        return H, C","metadata":{"_uuid":"e714d4c1-66dc-4d85-8925-0282979fdb01","_cell_guid":"c30e8113-93a1-48fe-9c4e-15c7824e6570","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-17T09:51:50.543741Z","iopub.execute_input":"2024-07-17T09:51:50.544586Z","iopub.status.idle":"2024-07-17T09:51:50.554999Z","shell.execute_reply.started":"2024-07-17T09:51:50.544536Z","shell.execute_reply":"2024-07-17T09:51:50.554018Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class ConvLSTM(nn.Module):\n\n    def __init__(self, in_channels, out_channels, \n    kernel_size, padding, activation, frame_size):\n\n        super(ConvLSTM, self).__init__()\n\n        self.out_channels = out_channels\n\n        # We will unroll this over time steps\n        self.convLSTMcell = ConvLSTMCell(in_channels, out_channels, \n        kernel_size, padding, activation, frame_size)\n\n    def forward(self, X):\n\n        # X is a frame sequence (batch_size, num_channels, seq_len, height, width)\n\n        # Get the dimensions\n        batch_size, _, seq_len, height, width = X.size()\n\n        # Initialize output\n        output = torch.zeros(batch_size, self.out_channels, seq_len, \n        height, width, device=device)\n        \n        # Initialize Hidden State\n        H = torch.zeros(batch_size, self.out_channels, \n        height, width, device=device)\n\n        # Initialize Cell Input\n        C = torch.zeros(batch_size,self.out_channels, \n        height, width, device=device)\n\n        # Unroll over time steps\n        for time_step in range(seq_len):\n\n            H, C = self.convLSTMcell(X[:,:,time_step], H, C)\n\n            output[:,:,time_step] = H\n\n        return output","metadata":{"_uuid":"039cd8c3-fc2b-42d9-8048-b7dc3048727d","_cell_guid":"82fd2175-61e7-4eae-93f9-931a9562e7e4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-17T09:51:50.556629Z","iopub.execute_input":"2024-07-17T09:51:50.557286Z","iopub.status.idle":"2024-07-17T09:51:50.572204Z","shell.execute_reply.started":"2024-07-17T09:51:50.557262Z","shell.execute_reply":"2024-07-17T09:51:50.569858Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n\n    def __init__(self, num_channels, num_kernels, kernel_size, padding, \n    activation, frame_size, num_layers):\n\n        super(Seq2Seq, self).__init__()\n\n        self.sequential = nn.Sequential()\n\n        # Add First layer (Different in_channels than the rest)\n        self.sequential.add_module(\n            \"convlstm1\", ConvLSTM(\n                in_channels=num_channels, out_channels=num_kernels,\n                kernel_size=kernel_size, padding=padding, \n                activation=activation, frame_size=frame_size)\n        )\n\n        self.sequential.add_module(\n            \"batchnorm1\", nn.BatchNorm3d(num_features=num_kernels)\n        ) \n\n        # Add rest of the layers\n        for l in range(2, num_layers+1):\n\n            self.sequential.add_module(\n                f\"convlstm{l}\", ConvLSTM(\n                    in_channels=num_kernels, out_channels=num_kernels,\n                    kernel_size=kernel_size, padding=padding, \n                    activation=activation, frame_size=frame_size)\n                )\n                \n            self.sequential.add_module(\n                f\"batchnorm{l}\", nn.BatchNorm3d(num_features=num_kernels)\n                ) \n\n        # Add Convolutional Layer to predict output frame\n        self.conv = nn.Conv2d(\n            in_channels=num_kernels, out_channels=num_channels,\n            kernel_size=kernel_size, padding=padding)\n\n    def forward(self, X):\n\n        # Forward propagation through all the layers\n        output = self.sequential(X)\n\n        # Return only the last output frame\n        output = self.conv(output[:,:,-1])\n        \n        return nn.Sigmoid()(output)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:51:50.575610Z","iopub.execute_input":"2024-07-17T09:51:50.576468Z","iopub.status.idle":"2024-07-17T09:51:50.595678Z","shell.execute_reply.started":"2024-07-17T09:51:50.576437Z","shell.execute_reply":"2024-07-17T09:51:50.590160Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Load Data as Numpy Array\n# MovingMNIST = np.load('/kaggle/input/mnist-moving/mnist_test_seq.npy')\nMovingMNIST = np.load('/kaggle/input/our-dummy-dataset/merged.npy')\nprint(MovingMNIST.shape)\n\n# MovingMNIST = MovingMNIST.transpose(1, 0, 2, 3)\n# print(MovingMNIST.shape)\n\n# Shuffle Data\nnp.random.shuffle(MovingMNIST)\n\n# Train, Test, Validation splits\n# train_data = MovingMNIST[:6000]         \n# val_data = MovingMNIST[8000:9000]       \n# test_data = MovingMNIST[9000:10000]\n\ntrain_data = MovingMNIST[:15]         \nval_data = MovingMNIST[15:17]       \ntest_data = MovingMNIST[17:20]     \n\ndef collate(batch):\n\n    # Add channel dim, scale pixels between 0 and 1, send to GPU\n    batch = torch.tensor(batch)\n#     batch = batch / 255.0                        \n    batch = batch.to(device)   \n\n    # Randomly pick 10 frames as input, 11th frame is target\n    rand = np.random.randint(10,15)                     \n    return batch[:,:,rand-10:rand], batch[:,:,rand]     \n\n\n# Training Data Loader\ntrain_loader = DataLoader(train_data, shuffle=True, \n                        batch_size=2, collate_fn=collate)\n\n# Validation Data Loader\nval_loader = DataLoader(val_data, shuffle=True, \n                        batch_size=2, collate_fn=collate)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:51:50.596857Z","iopub.execute_input":"2024-07-17T09:51:50.597757Z","iopub.status.idle":"2024-07-17T09:51:50.769404Z","shell.execute_reply.started":"2024-07-17T09:51:50.597715Z","shell.execute_reply":"2024-07-17T09:51:50.765949Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(20, 2, 18, 64, 64)\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Get a batch\n# input, _ = next(iter(val_loader))\n# print(input.shape)\n\n# # Reverse process before displaying\n# input = input.cpu().numpy() * 255.0     \n\n# for video in input.squeeze(1)[:3]:          # Loop over videos\n#     with io.BytesIO() as gif:\n#         imageio.mimsave(gif,video.astype(np.uint8),\"GIF\",fps=5)\n#         display(HBox([widgets.Image(value=gif.getvalue())]))","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:51:50.771952Z","iopub.execute_input":"2024-07-17T09:51:50.773700Z","iopub.status.idle":"2024-07-17T09:51:50.780837Z","shell.execute_reply.started":"2024-07-17T09:51:50.773656Z","shell.execute_reply":"2024-07-17T09:51:50.779403Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Get a batch\ninput, _ = next(iter(val_loader))\nprint(input.shape)\nprint(_.shape)\n\n# Reverse process before displaying\ninput = input.cpu().numpy() * 255.0\n\ninput = np.transpose(input, (0, 2, 3, 4, 1))\n\nfor video in input[:2]:  # Loop over first 3 videos\n    frames = []\n    for frame in video:\n        # Take only the first channel\n        frame = frame[:,:,0]\n        frames.append(frame.astype(np.uint8))\n    \n    with io.BytesIO() as gif:\n        imageio.mimsave(gif, frames, format=\"GIF\", fps=1)\n        display(HBox([widgets.Image(value=gif.getvalue())]))","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:51:50.788017Z","iopub.execute_input":"2024-07-17T09:51:50.789107Z","iopub.status.idle":"2024-07-17T09:51:51.296904Z","shell.execute_reply.started":"2024-07-17T09:51:50.789063Z","shell.execute_reply":"2024-07-17T09:51:51.296038Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/2760756042.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n  batch = torch.tensor(batch)\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([2, 2, 10, 64, 64])\ntorch.Size([2, 2, 64, 64])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x86\\x00\\x00\\x00\\x00\\x00%%%&&&\\'\\'\\'((()))***+++,,,---...///00011…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d13ecf34ae8f46dfa8463decd86ef37f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x86\\x00\\x00\\x10\\x10\\x10\\x11\\x11\\x11\\x12\\x12\\x12\\x13\\x13\\x13\\x15\\…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24fbd170019c4541aa3cfcc8dcca9a97"}},"metadata":{}}]},{"cell_type":"code","source":"# The input video frames are grayscale, thus single channel\nmodel = Seq2Seq(num_channels=2, num_kernels=64, \nkernel_size=(3, 3), padding=(1, 1), activation=\"relu\", \nframe_size=(64, 64), num_layers=6).to(device)\n\noptim = Adam(model.parameters(), lr=1e-4)\n\n# Binary Cross Entropy, target pixel values either 0 or 1\ncriterion = nn.BCEWithLogitsLoss(reduction='sum')","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:51:51.298177Z","iopub.execute_input":"2024-07-17T09:51:51.298493Z","iopub.status.idle":"2024-07-17T09:51:52.710027Z","shell.execute_reply.started":"2024-07-17T09:51:51.298468Z","shell.execute_reply":"2024-07-17T09:51:52.709116Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\nmin_val_loss = 100000\n\nnum_epochs = 10\nfor epoch in tqdm(range(1, num_epochs+1), desc=\"Epochs\"):\n    \n    train_loss = 0\n    model.train()\n    for batch_num, (input, target) in enumerate(train_loader, 1):\n        output = model(input)\n        loss = criterion(output.flatten(), target.flatten())\n        loss.backward()\n        optim.step()\n        optim.zero_grad()\n        train_loss += loss.item()\n    train_loss /= len(train_loader.dataset)\n    \n    val_loss = 0\n    model.eval()\n    with torch.no_grad():\n        for input, target in val_loader:\n            output = model(input)\n            loss = criterion(output.flatten(), target.flatten())\n            val_loss += loss.item()\n    val_loss /= len(val_loader.dataset)\n    \n    # save the model\n    if(val_loss < min_val_loss):\n        min_val_loss = val_loss\n        torch.save(model.state_dict(), \"/kaggle/working/model.pth\")\n    \n    print(f\"Epoch:{epoch} Training Loss:{train_loss:.2f} Validation Loss:{val_loss:.2f}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:51:52.711145Z","iopub.execute_input":"2024-07-17T09:51:52.711548Z","iopub.status.idle":"2024-07-17T09:52:19.426587Z","shell.execute_reply.started":"2024-07-17T09:51:52.711522Z","shell.execute_reply":"2024-07-17T09:52:19.425659Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Epochs:  10%|█         | 1/10 [00:03<00:35,  3.92s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:1 Training Loss:6334.54 Validation Loss:6885.02\n\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  20%|██        | 2/10 [00:06<00:24,  3.11s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:2 Training Loss:5791.56 Validation Loss:6881.19\n\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  30%|███       | 3/10 [00:08<00:19,  2.85s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:3 Training Loss:5719.89 Validation Loss:6786.67\n\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  40%|████      | 4/10 [00:11<00:16,  2.75s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:4 Training Loss:5697.52 Validation Loss:6281.96\n\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  50%|█████     | 5/10 [00:14<00:13,  2.70s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:5 Training Loss:5687.28 Validation Loss:5867.89\n\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  60%|██████    | 6/10 [00:16<00:10,  2.64s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:6 Training Loss:5683.39 Validation Loss:5779.40\n\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  70%|███████   | 7/10 [00:19<00:07,  2.59s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:7 Training Loss:5682.21 Validation Loss:5780.77\n\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  80%|████████  | 8/10 [00:21<00:05,  2.56s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:8 Training Loss:5681.52 Validation Loss:5783.33\n\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  90%|█████████ | 9/10 [00:24<00:02,  2.54s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:9 Training Loss:5681.10 Validation Loss:5846.59\n\n","output_type":"stream"},{"name":"stderr","text":"Epochs: 100%|██████████| 10/10 [00:26<00:00,  2.67s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch:10 Training Loss:5682.50 Validation Loss:5822.47\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"def collate_test(batch):\n\n    # Last 10 frames are target\n    target = np.array(batch)[:,:,10:]       \n    \n    # Add channel dim, scale pixels between 0 and 1, send to GPU\n#     batch = torch.tensor(batch).unsqueeze(1)          \n    batch = torch.tensor(batch) \n#     batch = batch / 255.0                             \n    batch = batch.to(device)                          \n    return batch, target\n\n# Test Data Loader\ntest_loader = DataLoader(test_data,shuffle=True, \n                         batch_size=2, collate_fn=collate_test)\n\n# Get a batch\nbatch, target = next(iter(test_loader))\nprint(batch.shape)\nprint(target.shape)\n\n# Initialize output sequence\noutput = np.zeros(target.shape, dtype=np.uint8)\n\n# Loop over timesteps\nwith torch.no_grad():\n    for timestep in range(target.shape[1]):\n      input = batch[:,:,timestep:timestep+10]   \n      output[:,:,timestep] = (model(input).detach().cpu() * 255.0).numpy()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:52:19.429310Z","iopub.execute_input":"2024-07-17T09:52:19.429607Z","iopub.status.idle":"2024-07-17T09:52:19.649161Z","shell.execute_reply.started":"2024-07-17T09:52:19.429583Z","shell.execute_reply":"2024-07-17T09:52:19.648476Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"torch.Size([2, 2, 18, 64, 64])\n(2, 2, 8, 64, 64)\n","output_type":"stream"}]},{"cell_type":"code","source":"target = np.transpose(target, (0, 2, 3, 4, 1))\noutput = np.transpose(output, (0, 2, 3, 4, 1))\n\nfor tgt, out in zip(target, output):       # Loop over samples\n    # Write target video as gif\n    \n    tgt = tgt * 255.0\n    \n    frames = []\n    for frame in tgt:\n        # Take only the first channel\n        frame = frame[:,:,0]\n        frames.append(frame.astype(np.uint8))\n        \n    with io.BytesIO() as gif:\n        imageio.mimsave(gif, frames, \"GIF\", fps = 1)    \n        target_gif = gif.getvalue()\n\n    # Write output video as gif\n    frames = []\n    for frame in out:\n        # Take only the first channel\n        frame = frame[:,:,0]\n        frames.append(frame.astype(np.uint8))\n        \n    with io.BytesIO() as gif:\n        imageio.mimsave(gif, frames, \"GIF\", fps = 1)    \n        output_gif = gif.getvalue()\n\n    display(HBox([widgets.Image(value=target_gif), \n                  widgets.Image(value=output_gif)]))","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:52:19.650195Z","iopub.execute_input":"2024-07-17T09:52:19.650490Z","iopub.status.idle":"2024-07-17T09:52:19.692751Z","shell.execute_reply.started":"2024-07-17T09:52:19.650467Z","shell.execute_reply":"2024-07-17T09:52:19.691866Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x86\\x00\\x00\\x02\\x02\\x02\\x03\\x03\\x03\\x04\\x04\\x04\\x05\\x05\\x05\\x06\\…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"996784a0cb9c4ae6b311bb7d3d19cd81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x87\\x00\\x00\\x0f\\x0f\\x0f\\x11\\x11\\x11\\x12\\x12\\x12\\x13\\x13\\x13\\x14\\…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"468494152d1c4c26a8d1118dd82050a6"}},"metadata":{}}]}]}
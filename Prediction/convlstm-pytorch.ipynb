{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8905724,"sourceType":"datasetVersion","datasetId":5354472},{"sourceId":9041980,"sourceType":"datasetVersion","datasetId":5398602}],"dockerImageVersionId":30733,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nimport numpy as np\n\nimport io\nimport imageio\nfrom ipywidgets import widgets, HBox\n\n# Use GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-07-27T06:36:14.335601Z","iopub.execute_input":"2024-07-27T06:36:14.335947Z","iopub.status.idle":"2024-07-27T06:36:18.130343Z","shell.execute_reply.started":"2024-07-27T06:36:14.335919Z","shell.execute_reply":"2024-07-27T06:36:18.129296Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Original ConvLSTM cell as proposed by Shi et al.\nclass ConvLSTMCell(nn.Module):\n\n    def __init__(self, in_channels, out_channels, \n    kernel_size, padding, activation, frame_size):\n\n        super(ConvLSTMCell, self).__init__()  \n\n        if activation == \"tanh\":\n            self.activation = torch.tanh \n        elif activation == \"relu\":\n            self.activation = torch.relu\n        \n        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n        self.conv = nn.Conv2d(\n            in_channels=in_channels + out_channels, \n            out_channels=4 * out_channels, \n            kernel_size=kernel_size, \n            padding=padding)           \n\n        # Initialize weights for Hadamard Products\n        self.W_ci = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n        self.W_co = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n        self.W_cf = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n\n    def forward(self, X, H_prev, C_prev):\n\n        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n        conv_output = self.conv(torch.cat([X, H_prev], dim=1))\n\n        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n        i_conv, f_conv, C_conv, o_conv = torch.chunk(conv_output, chunks=4, dim=1)\n\n        input_gate = torch.sigmoid(i_conv + self.W_ci * C_prev )\n        forget_gate = torch.sigmoid(f_conv + self.W_cf * C_prev )\n\n        # Current Cell output\n        C = forget_gate*C_prev + input_gate * self.activation(C_conv)\n\n        output_gate = torch.sigmoid(o_conv + self.W_co * C )\n\n        # Current Hidden State\n        H = output_gate * self.activation(C)\n\n        return H, C","metadata":{"_uuid":"e714d4c1-66dc-4d85-8925-0282979fdb01","_cell_guid":"c30e8113-93a1-48fe-9c4e-15c7824e6570","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-27T06:36:18.132310Z","iopub.execute_input":"2024-07-27T06:36:18.132716Z","iopub.status.idle":"2024-07-27T06:36:18.143469Z","shell.execute_reply.started":"2024-07-27T06:36:18.132689Z","shell.execute_reply":"2024-07-27T06:36:18.142321Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class ConvLSTM(nn.Module):\n\n    def __init__(self, in_channels, out_channels, \n    kernel_size, padding, activation, frame_size):\n\n        super(ConvLSTM, self).__init__()\n\n        self.out_channels = out_channels\n\n        # We will unroll this over time steps\n        self.convLSTMcell = ConvLSTMCell(in_channels, out_channels, \n        kernel_size, padding, activation, frame_size)\n\n    def forward(self, X):\n\n        # X is a frame sequence (batch_size, num_channels, seq_len, height, width)\n\n        # Get the dimensions\n        batch_size, _, seq_len, height, width = X.size()\n\n        # Initialize output\n        output = torch.zeros(batch_size, self.out_channels, seq_len, \n        height, width, device=device)\n        \n        # Initialize Hidden State\n        H = torch.zeros(batch_size, self.out_channels, \n        height, width, device=device)\n\n        # Initialize Cell Input\n        C = torch.zeros(batch_size,self.out_channels, \n        height, width, device=device)\n\n        # Unroll over time steps\n        for time_step in range(seq_len):\n\n            H, C = self.convLSTMcell(X[:,:,time_step], H, C)\n\n            output[:,:,time_step] = H\n\n        return output","metadata":{"_uuid":"039cd8c3-fc2b-42d9-8048-b7dc3048727d","_cell_guid":"82fd2175-61e7-4eae-93f9-931a9562e7e4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-27T06:36:18.144869Z","iopub.execute_input":"2024-07-27T06:36:18.145513Z","iopub.status.idle":"2024-07-27T06:36:18.156414Z","shell.execute_reply.started":"2024-07-27T06:36:18.145474Z","shell.execute_reply":"2024-07-27T06:36:18.155548Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n\n    def __init__(self, num_channels, num_kernels, kernel_size, padding, \n    activation, frame_size, num_layers):\n\n        super(Seq2Seq, self).__init__()\n\n        self.sequential = nn.Sequential()\n\n        # Add First layer (Different in_channels than the rest)\n        self.sequential.add_module(\n            \"convlstm1\", ConvLSTM(\n                in_channels=num_channels, out_channels=num_kernels,\n                kernel_size=kernel_size, padding=padding, \n                activation=activation, frame_size=frame_size)\n        )\n\n        self.sequential.add_module(\n            \"batchnorm1\", nn.BatchNorm3d(num_features=num_kernels)\n        ) \n\n        # Add rest of the layers\n        for l in range(2, num_layers+1):\n\n            self.sequential.add_module(\n                f\"convlstm{l}\", ConvLSTM(\n                    in_channels=num_kernels, out_channels=num_kernels,\n                    kernel_size=kernel_size, padding=padding, \n                    activation=activation, frame_size=frame_size)\n                )\n                \n            self.sequential.add_module(\n                f\"batchnorm{l}\", nn.BatchNorm3d(num_features=num_kernels)\n                ) \n\n        # Add Convolutional Layer to predict output frame\n        self.conv = nn.Conv2d(\n            in_channels=num_kernels, out_channels=num_channels,\n            kernel_size=kernel_size, padding=padding)\n\n    def forward(self, X):\n\n        # Forward propagation through all the layers\n        output = self.sequential(X)\n\n        # Return only the last output frame\n        output = self.conv(output[:,:,-1])\n        \n        return nn.Sigmoid()(output)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T06:36:18.159322Z","iopub.execute_input":"2024-07-27T06:36:18.159709Z","iopub.status.idle":"2024-07-27T06:36:18.170873Z","shell.execute_reply.started":"2024-07-27T06:36:18.159684Z","shell.execute_reply":"2024-07-27T06:36:18.169858Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Load Data as Numpy Array\n# MovingMNIST = np.load('/kaggle/input/mnist-moving/mnist_test_seq.npy')\nMovingMNIST = np.load('/kaggle/input/our-dummy-dataset/merged.npy')\nprint(MovingMNIST.shape)\n\n# MovingMNIST = MovingMNIST.transpose(1, 0, 2, 3)\n# print(MovingMNIST.shape)\n\n# Shuffle Data\nnp.random.shuffle(MovingMNIST)\n\n# 70:15:15 split\nn = len(MovingMNIST)\ntrain_data, val_data, test_data = MovingMNIST[:int(0.7*n)], MovingMNIST[int(0.7*n):int(0.85*n)], MovingMNIST[int(0.85*n):]\n\nf = 15 # total frames for training\ndef collate(batch):\n\n    # Add channel dim, scale pixels between 0 and 1, send to GPU\n    batch = torch.tensor(batch)\n#     batch = batch / 255.0                        \n    batch = batch.to(device)   \n    \n    # Randomly pick f frames as input, (f+1)th frame is target\n    rand = np.random.randint(0, batch.shape[2] - f)  # Ensure there are enough frames\n    return batch[:, :, rand:rand+f], batch[:, :, rand+f]\n\n# Training Data Loader\ntrain_loader = DataLoader(train_data, shuffle=True, \n                        batch_size=16, collate_fn=collate)\n\n# Validation Data Loader\nval_loader = DataLoader(val_data, shuffle=True, \n                        batch_size=16, collate_fn=collate)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T06:36:18.171927Z","iopub.execute_input":"2024-07-27T06:36:18.172203Z","iopub.status.idle":"2024-07-27T06:36:22.616056Z","shell.execute_reply.started":"2024-07-27T06:36:18.172179Z","shell.execute_reply":"2024-07-27T06:36:22.615126Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(587, 2, 20, 64, 64)\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Get a batch\n# input, _ = next(iter(val_loader))\n# print(input.shape)\n\n# # Reverse process before displaying\n# input = input.cpu().numpy() * 255.0     \n\n# for video in input.squeeze(1)[:3]:          # Loop over videos\n#     with io.BytesIO() as gif:\n#         imageio.mimsave(gif,video.astype(np.uint8),\"GIF\",fps=5)\n#         display(HBox([widgets.Image(value=gif.getvalue())]))","metadata":{"execution":{"iopub.status.busy":"2024-07-27T06:36:22.617260Z","iopub.execute_input":"2024-07-27T06:36:22.617544Z","iopub.status.idle":"2024-07-27T06:36:22.621866Z","shell.execute_reply.started":"2024-07-27T06:36:22.617520Z","shell.execute_reply":"2024-07-27T06:36:22.620840Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Get a batch\ninput, _ = next(iter(val_loader))\nprint(input.shape)\nprint(_.shape)\n\n# Reverse process before displaying\ninput = input.cpu().numpy() * 255.0\n\ninput = np.transpose(input, (0, 2, 3, 4, 1))\n\nfor video in input[:2]:  # Loop over first 3 videos\n    frames = []\n    for frame in video:\n        # Take only the first channel\n        frame = frame[:,:,0]\n        frames.append(frame.astype(np.uint8))\n    \n    with io.BytesIO() as gif:\n        imageio.mimsave(gif, frames, format=\"GIF\", fps=1)\n        display(HBox([widgets.Image(value=gif.getvalue())]))","metadata":{"execution":{"iopub.status.busy":"2024-07-27T06:36:22.623047Z","iopub.execute_input":"2024-07-27T06:36:22.623371Z","iopub.status.idle":"2024-07-27T06:36:23.719419Z","shell.execute_reply.started":"2024-07-27T06:36:22.623342Z","shell.execute_reply":"2024-07-27T06:36:23.718580Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/2379469153.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n  batch = torch.tensor(batch)\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([16, 2, 15, 64, 64])\ntorch.Size([16, 2, 64, 64])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x86\\x00\\x00BBBCCCJJJMMMNNNOOOPPPQQQRRRSSSTTTUUUVVVWWWXXXYYYZZZ[[…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c26b486601b2473ea1944c37fd577cc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x84\\x00\\x00\\x7f\\x7f\\x7f\\x80\\x80\\x80\\x81\\x81\\x81\\x82\\x82\\x82\\x83\\…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65f69e697c594eddb87648802f5b70d7"}},"metadata":{}}]},{"cell_type":"code","source":"# The input video frames are grayscale, thus single channel\nmodel = Seq2Seq(num_channels=2, num_kernels=64, \nkernel_size=(3, 3), padding=(1, 1), activation=\"relu\", \nframe_size=(64, 64), num_layers=6).to(device)\n\noptim = Adam(model.parameters(), lr=1e-4)\n\n# Binary Cross Entropy, target pixel values either 0 or 1\ncriterion = nn.BCEWithLogitsLoss(reduction='sum')","metadata":{"execution":{"iopub.status.busy":"2024-07-27T06:46:57.716843Z","iopub.execute_input":"2024-07-27T06:46:57.717489Z","iopub.status.idle":"2024-07-27T06:46:57.753500Z","shell.execute_reply.started":"2024-07-27T06:46:57.717460Z","shell.execute_reply":"2024-07-27T06:46:57.752591Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# clears gpu memory\nimport gc\ngc.collect()\n\nwith torch.no_grad():\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T06:46:57.755202Z","iopub.execute_input":"2024-07-27T06:46:57.755491Z","iopub.status.idle":"2024-07-27T06:46:58.269444Z","shell.execute_reply.started":"2024-07-27T06:46:57.755467Z","shell.execute_reply":"2024-07-27T06:46:58.268594Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nmin_val_loss = 100000\nnum_epochs = 2\n\ntrain_losses = []\nval_losses = []\n\nfor epoch in tqdm(range(1, num_epochs+1), desc=\"Epochs\"):\n    train_loss = 0\n    model.train()\n    for batch_num, (input, target) in enumerate(train_loader, 1):\n        output = model(input)\n        loss = criterion(output.flatten(), target.flatten())\n        loss.backward()\n        optim.step()\n        optim.zero_grad()\n        train_loss += loss.item()\n    \n    train_loss /= len(train_loader.dataset)\n    train_losses.append(train_loss)\n    \n    val_loss = 0\n    model.eval()\n    with torch.no_grad():\n        for input, target in val_loader:\n            output = model(input)\n            loss = criterion(output.flatten(), target.flatten())\n            val_loss += loss.item()\n    \n    val_loss /= len(val_loader.dataset)\n    val_losses.append(val_loss)\n    \n    # save the model\n    if val_loss < min_val_loss:\n        min_val_loss = val_loss\n        torch.save(model.state_dict(), \"/kaggle/working/model.pth\")\n    \n    print(f\"Epoch:{epoch} Training Loss:{train_loss:.2f} Validation Loss:{val_loss:.2f}\\n\")\n    \n    # Plot loss graph\n    plt.figure(figsize=(10, 5))\n    plt.plot(range(1, epoch+1), train_losses, label='Training Loss')\n    plt.plot(range(1, epoch+1), val_losses, label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T06:46:58.271100Z","iopub.execute_input":"2024-07-27T06:46:58.271506Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Epochs:   0%|          | 0/2 [00:00<?, ?it/s]","output_type":"stream"}]},{"cell_type":"code","source":"def collate_test(batch):\n\n    # Last 5 frames are target\n    target = np.array(batch)[:,:,-5:]       \n    \n    # Add channel dim, scale pixels between 0 and 1, send to GPU\n#     batch = torch.tensor(batch).unsqueeze(1)          \n    batch = torch.tensor(batch) \n#     batch = batch / 255.0                             \n    batch = batch.to(device)                          \n    return batch, target\n\n# Test Data Loader\ntest_loader = DataLoader(test_data,shuffle=True, \n                         batch_size=3, collate_fn=collate_test)\n\n# Get a batch\nbatch, target = next(iter(test_loader))\nprint(batch.shape)\nprint(target.shape)\n\n# Initialize output sequence\noutput = np.zeros(target.shape, dtype=np.uint8)\n\n# Loop over timesteps\nwith torch.no_grad():\n    for timestep in range(target.shape[2]):\n      input = batch[:,:,timestep:timestep+15]   \n      output[:,:,timestep] = (model(input).detach().cpu() * 255.0).numpy()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = np.transpose(target, (0, 2, 3, 4, 1))\noutput = np.transpose(output, (0, 2, 3, 4, 1))\n\nfor tgt, out in zip(target, output):       # Loop over samples\n    # Write target video as gif\n    \n    tgt = tgt * 255.0\n    \n    frames = []\n    for frame in tgt:\n        # Take only the first channel\n        frame = frame[:,:,0]\n        frames.append(frame.astype(np.uint8))\n        \n    with io.BytesIO() as gif:\n        imageio.mimsave(gif, frames, \"GIF\", fps = 1)    \n        target_gif = gif.getvalue()\n\n    # Write output video as gif\n    frames = []\n    for frame in out:\n        # Take only the first channel\n        frame = frame[:,:,0]\n        frames.append(frame.astype(np.uint8))\n        \n    with io.BytesIO() as gif:\n        imageio.mimsave(gif, frames, \"GIF\", fps = 1)    \n        output_gif = gif.getvalue()\n\n    display(HBox([widgets.Image(value=target_gif), \n                  widgets.Image(value=output_gif)]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}